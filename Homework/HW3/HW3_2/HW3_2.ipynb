{"cells":[{"cell_type":"markdown","metadata":{"id":"eB0lBzkJy_l6"},"source":["# HW3.2: Neural Transition-Based Dependency Parsing\n"]},{"cell_type":"markdown","metadata":{"id":"hwbJPnBmy_l-"},"source":["In this exercise, you are going to build a deep learning model for Neural Networks Transition-Based Dependency Parsing. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between “head” words and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time."]},{"cell_type":"markdown","metadata":{"id":"iTop7w0Wy_mT"},"source":["To complete this exercise, you will need to complete the code and build a deep learning model for dependency parsing. \n","\n","We provide the code for data preparation and the skeleton for PartialParse class. You do not need to understand the code outside of this notebook. \n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10862,"status":"ok","timestamp":1679649069061,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"NUwCUisbzOAb","outputId":"65193def-4aa6-41ef-fa6d-eb01879f5fd2"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25677,"status":"ok","timestamp":1679649094724,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"2fJ8cYFqzPS9","outputId":"cb0f4d96-f65f-4ee9-d5f8-3a6cb57780bc"},"outputs":[],"source":["# import shutil\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/HW3-2.zip\", \"/content/HW3-2.zip\")\n","# !unzip -q HW3-2.zip"]},{"cell_type":"markdown","metadata":{"id":"lzF49xKny_mX"},"source":["## 1. Transition-Based Dependency Parsing"]},{"cell_type":"markdown","metadata":{"id":"PwH68mjMy_mY"},"source":["Your implementation will be a transition-based parser, which incrementally builds\n","up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:\n","- A stack of words that are currently being processed.\n","- A buffer of words yet to be processed.\n","- A list of dependencies predicted by the parser.\n","\n","Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words\n","of the sentence in order. At each step, the parse applies a transition to the partial parse until its buffer is\n","empty and the stack is size 1. The following transitions can be applied:\n","- SHIFT: removes the first word from the buffer and pushes it onto the stack.\n","- LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the\n","first item and removes the second item from the stack.\n","- RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second\n","item and removes the first item from the stack.\n","\n","Your parser will decide among transitions at each state using a neural network classifier."]},{"cell_type":"markdown","metadata":{"id":"Ab3kn2OVy_mb"},"source":["### TODO 1 (Written):\n","Go through the sequence of transitions needed for parsing the sentence “I parsed\n","this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the\n","configuration of the stack and buffer, as well as what transition was applied this step and what new\n","dependency was added (if any). The first three steps are provided below as an example."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aTkSbSf6y_ma"},"source":["Image --> https://drive.google.com/file/d/10jYgxDhsyolZGarcNTEdt6G2xB0l9iZU/view?usp=share_link  \n","\n","![TODO 1 IMG](pic/HW3-2_img1.jpg)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OPmRQbXky_mc"},"source":["Complete the following table (double click the table and fill in the rest):\n","\n","| stack                             |  buffer                                   |  new dependency   | transition            |\n","| :------:                          | :------:                                  | :-------------:   | :--------:            |\n","| \\[ROOT\\]                          | \\[I, parsed, this, sentence, correctly\\]  |                   | Initial Configuration |\n","| \\[ROOT, I\\]                       | \\[parsed, this, sentence, correctly\\]     |                   | SHIFT                 |\n","| \\[ROOT, I, parsed\\]               | \\[this, sentence, correctly\\]             |                   | SHIFT                 |\n","| \\[ROOT, parsed\\]                  | \\[this, sentence, correctly\\]             | parsed→I          | LEFT-ARC              |\n","| \\[ROOT, parsed, this\\]            | \\[sentence, correctly\\]                   |                   | SHIFT                 |\n","| \\[ROOT, parsed, this, sentence\\]  | \\[correctly\\]                             |                   | SHIFT                 |\n","| \\[ROOT, parsed, sentence\\]        | \\[correctly\\]                             | sentence→this     | LEFT-ARC              |\n","| \\[ROOT, parsed\\]                  | \\[correctly\\]                             | parsed→sentence   | RIGHT-ARC             |\n","| \\[ROOT, parsed, correctly\\]       | \\[\\]                                      |                   | SHIFT                 |\n","| \\[ROOT, parsed\\]                  | \\[\\]                                      | parsed→correctly  | RIGHT-ARC             |\n","| \\[ROOT\\]                          | \\[\\]                                      | ROOT→parsed       | RIGHT-ARC             |\n","\n","References:\n","- https://medium.com/mlearning-ai/dependency-parsing-with-neural-networks-e36f5166628d\n","- https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf"]},{"cell_type":"markdown","metadata":{"id":"-h6PmOd6y_me"},"source":["### TODO 2 (Coding):\n","Implement the __\\_\\_init\\_\\___ and __parse_step__ functions in the PartialParse class. Your code must past both of the following tests."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"WXhOjsN_y_mf"},"outputs":[],"source":["class PartialParse(object):\n","    def __init__(self, sentence):\n","        \"\"\"Initializes this partial parse.\n","\n","        Your code should initialize the following fields:\n","            self.stack: The current stack represented as a list with the top of the stack as the\n","                        last element of the list.\n","            self.buffer: The current buffer represented as a list with the first item on the\n","                         buffer as the first item of the list\n","            self.dependencies: The list of dependencies produced so far. Represented as a list of\n","                    tuples where each tuple is of the form (head, dependent).\n","                    Order for this list doesn't matter.\n","\n","        The root token should be represented with the string \"ROOT\"\n","\n","        Args:\n","            sentence: The sentence to be parsed as a list of words.\n","                      Your code should not modify the sentence.\n","        \"\"\"\n","        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n","        self.sentence = sentence #--list\n","\n","        ### YOUR CODE HERE\n","        self.stack = ['ROOT']  # --> list\n","        self.buffer = self.sentence.copy() # --> list\n","        self.dependencies = []  # --> list\n","        ### END YOUR CODE\n","\n","    def parse_step(self, transition):\n","        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n","\n","        Args:\n","            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n","                        and right-arc transitions. You can assume the provided transition is a legal\n","                        transition.\n","        \"\"\"\n","        ### YOUR CODE HERE\n","        if transition == 'S':\n","            self.stack.append(self.buffer.pop(0))\n","        if transition == 'LA':\n","            self.dependencies.append((self.stack[-1], self.stack[-2]))\n","            self.stack.pop(-2)\n","        if transition == 'RA':\n","            self.dependencies.append((self.stack[-2], self.stack[-1]))\n","            self.stack.pop(-1)\n","        ### END YOUR CODE\n","\n","    def parse(self, transitions):\n","        \"\"\"Applies the provided transitions to this PartialParse\n","\n","        Args:\n","            transitions: The list of transitions in the order they should be applied\n","        Returns:\n","            dependencies: The list of dependencies produced when parsing the sentence. Represented\n","                          as a list of tuples where each tuple is of the form (head, dependent)\n","        \"\"\"\n","        for transition in transitions:\n","            self.parse_step(transition)\n","        return self.dependencies"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_kF_8ESPy_mk"},"outputs":[],"source":["# Do not modify this code\n","def test_step(name, transition, stack, buf, deps,\n","              ex_stack, ex_buf, ex_deps):\n","    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n","    pp = PartialParse([])\n","    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n","\n","    pp.parse_step(transition)\n","    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n","    assert stack == ex_stack, \\\n","        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n","    assert buf == ex_buf, \\\n","        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n","    assert deps == ex_deps, \\\n","        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n","    print(\"{:} test passed!\".format(name))\n","\n","\n","def test_parse_step():\n","    \"\"\"Simple tests for the PartialParse.parse_step function\n","    Warning: these are not exhaustive\n","    \"\"\"\n","    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n","              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n","    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n","              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n","    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n","              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n","\n","\n","def test_parse():\n","    \"\"\"Simple tests for the PartialParse.parse function\n","    Warning: these are not exhaustive\n","    \"\"\"\n","    sentence = [\"parse\", \"this\", \"sentence\"]\n","    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n","    dependencies = tuple(sorted(dependencies))\n","    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n","    assert dependencies == expected,  \\\n","        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n","    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n","        \"parse test failed: the input sentence should not be modified\"\n","    print(\"parse test passed!\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MwNZjFgey_mn"},"outputs":[{"name":"stdout","output_type":"stream","text":["SHIFT test passed!\n","LEFT-ARC test passed!\n","RIGHT-ARC test passed!\n","parse test passed!\n"]}],"source":["test_parse_step()\n","test_parse()"]},{"cell_type":"markdown","metadata":{"id":"EL5kIWKXy_m4"},"source":["## 2. Setup and Preprocessing"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1679637678758,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"zemNby1uy_m5"},"outputs":[],"source":["from utils.parser_utils import minibatches, load_and_preprocess_data"]},{"cell_type":"markdown","metadata":{"id":"RksEEdJvy_m8"},"source":["Preparing data. We will use a subset of Penn Treebank and pretrained embeddings in this task"]},{"cell_type":"markdown","metadata":{"id":"5XNgGpMUy_m9"},"source":["We are now going to train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next. First, the model extracts a feature vector representing the current state. We will be using the feature set presented in the original neural dependency parsing paper: A Fast and Accurate Dependency Parser using Neural Networks. \n","\n","The function extracting these features has been implemented for you in parser_utils. This feature vector consists of a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.). They can be represented as a list of integers."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5752,"status":"ok","timestamp":1679637686543,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"JhqbGTYpy_m-","outputId":"c67f8096-2c13-4e42-b52c-ae2521f5d45e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","took 1.96 seconds\n","Building parser...\n","took 0.02 seconds\n","Loading pretrained embeddings...\n","took 2.23 seconds\n","Vectorizing data...\n","took 0.08 seconds\n","Preprocessing training data...\n","took 1.15 seconds\n"]}],"source":["parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(True)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1679637686543,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"D-VYL2rKy_nB","outputId":"20a722e9-44e1-4c71-a820-30ab47e60820"},"outputs":[{"name":"stdout","output_type":"stream","text":["48390 500 500\n"]}],"source":["print(len(train_examples), len(dev_set), len(test_set))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1679637686544,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"cUGa5-KldWCD","outputId":"68039d8f-94b4-4279-8c12-4ebe091c42a6"},"outputs":[{"data":{"text/plain":["([5156,\n","  660,\n","  88,\n","  96,\n","  85,\n","  2131,\n","  5155,\n","  5155,\n","  5155,\n","  5155,\n","  5155,\n","  5155,\n","  91,\n","  5155,\n","  113,\n","  5155,\n","  5155,\n","  5155,\n","  84,\n","  39,\n","  40,\n","  61,\n","  41,\n","  39,\n","  83,\n","  83,\n","  83,\n","  83,\n","  83,\n","  83,\n","  40,\n","  83,\n","  41,\n","  83,\n","  83,\n","  83],\n"," [1, 1, 1],\n"," 2)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_examples[10]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1679637686544,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Yo_qz2-11MvG","outputId":"776511de-28c0-4d36-ea01-274b5f9b6825"},"outputs":[{"data":{"text/plain":["array([[-0.35588855,  0.6878136 , -0.27288294, ...,  1.2894735 ,\n","        -0.8015917 , -0.832532  ],\n","       [ 1.5229279 , -0.14771435,  0.64237845, ...,  0.1900125 ,\n","         0.5338304 , -0.9380984 ],\n","       [-2.1680155 , -0.7834329 ,  0.47289106, ..., -0.74451214,\n","        -1.5166008 ,  0.02513922],\n","       ...,\n","       [ 0.50952065, -0.4281435 ,  0.51560694, ..., -0.83727574,\n","         0.581582  ,  2.9018507 ],\n","       [-1.3396579 ,  0.47578225,  0.8213765 , ..., -0.39435554,\n","         0.35004184,  1.0678409 ],\n","       [-1.016677  ,  0.6115748 , -0.4053425 , ..., -1.1876915 ,\n","         0.02993344, -0.4450842 ]], dtype=float32)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["embeddings"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1679637686544,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"FubWDIsNy_nE","outputId":"9a229b71-f9d4-473d-b928-3f7fb2a1e4d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5157, 50)\n"]}],"source":["print(embeddings.shape)"]},{"cell_type":"markdown","metadata":{"id":"ZBD3A4yVy_nI"},"source":["Get the full batch of our subset data"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679637686545,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"8vNwux9by_nJ"},"outputs":[],"source":["minibatch_gen = minibatches(train_examples, len(train_examples))\n","x_train, y_train = minibatch_gen.__next__()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1679637686545,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"i__8jliGy_nM","outputId":"74ed1a72-4183-4494-972f-7e93f456f516"},"outputs":[{"name":"stdout","output_type":"stream","text":["(48390, 36)\n","(48390, 3)\n"]}],"source":["print(x_train.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679637745105,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"ls7ApWjlNgOz","outputId":"3624500c-a7d4-46c7-e5ac-0bd54a71b08b"},"outputs":[{"data":{"text/plain":["array([ 111,  113,  578,   88,  140, 1239, 1746, 5155, 5155, 5155, 5155,\n","       5155, 5155, 5155, 5155, 5155, 5155, 5155,   40,   41,   39,   40,\n","         40,   49,   43,   83,   83,   83,   83,   83,   83,   83,   83,\n","         83,   83,   83])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["x_train[0]"]},{"cell_type":"markdown","metadata":{"id":"LbVsKG-rOFdp"},"source":["You can use parser.id2tok[word_id] to lookup the word in English."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679637860711,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"P_vlptDWNEs_","outputId":"6c2766be-8dee-4b20-8764-a9de31c3e09e"},"outputs":[{"name":"stdout","output_type":"stream","text":["with\n","an\n","offer\n","of\n","about\n","900\n","initial\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<NULL>\n","<p>:IN\n","<p>:DT\n","<p>:NN\n","<p>:IN\n","<p>:IN\n","<p>:CD\n","<p>:JJ\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n","<p>:<NULL>\n"]}],"source":["for word_id in x_train[0]:\n","  print(parser.id2tok[word_id])"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679591605715,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"OEJeDWvkclRS","outputId":"e4b263e7-a508-4338-ed65-8803ac0635f6"},"outputs":[{"data":{"text/plain":["array([1., 0., 0.])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["y_train[0]"]},{"cell_type":"markdown","metadata":{"id":"17eqeESxy_nR"},"source":["## 3. Model"]},{"cell_type":"code","execution_count":194,"metadata":{"id":"T5mqcz1qy_nT"},"outputs":[],"source":["from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense, Dropout, Flatten\n","from tensorflow.keras.optimizers import Adam\n","from kerastuner.tuners import RandomSearch\n","from tensorflow.random import set_seed\n","set_seed(2)"]},{"cell_type":"markdown","metadata":{"id":"5sW29Rhvy_nX"},"source":["### TODO 3 (Coding):\n","Build and train a tensroflow keras model to predict an action for each state of of the input. This is a simple classification task. \n","- The input and output of the model must match the dimention of x_train and y_train.\n","- The model must use the provided pretrained embeddings\n","- The model could comprise of only a feedforward layer and a dropout\n","- Training loss should be around 0.1 or below, and training categorical_accuracy above 0.94"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"jAWlQI6iy_nY"},"outputs":[],"source":["# model = Sequential()\n","# # Write your code here\n","# model.add(Embedding(input_dim=embeddings.shape[0], output_dim=embeddings.shape[1], weights=[embeddings], trainable=False))\n","# model.add(Dense(128, activation='relu'))\n","# model.add(Dropout(0.5))\n","# model.add(Dense(3, activation='softmax'))\n","# model.add(Flatten())\n","# model.add(Reshape((3,)))\n","\n","# model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['categorical_accuracy'])"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/plain":["(48390, 36)"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["x_train.shape"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"data":{"text/plain":["(48390, 3)"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["y_train.shape"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"data":{"text/plain":["(5157, 50)"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["embeddings.shape"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_40\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_52 (Embedding)    (None, 36, 50)            257850    \n","                                                                 \n"," dense_85 (Dense)            (None, 36, 128)           6528      \n","                                                                 \n"," dense_86 (Dense)            (None, 36, 64)            8256      \n","                                                                 \n"," dense_87 (Dense)            (None, 36, 64)            4160      \n","                                                                 \n"," dense_88 (Dense)            (None, 36, 32)            2080      \n","                                                                 \n"," dropout_36 (Dropout)        (None, 36, 32)            0         \n","                                                                 \n"," flatten_19 (Flatten)        (None, 1152)              0         \n","                                                                 \n"," dense_89 (Dense)            (None, 3)                 3459      \n","                                                                 \n","=================================================================\n","Total params: 282,333\n","Trainable params: 24,483\n","Non-trainable params: 257,850\n","_________________________________________________________________\n"]}],"source":["# def create_model(embeddings, x_shape, y_shape):\n","#     model = Sequential()\n","#     model.add(Input(shape=(x_shape,)))\n","#     model.add(Embedding(input_dim=embeddings.shape[0], output_dim=embeddings.shape[1], trainable=False))\n","#     model.add(Dense(128, activation='relu'))\n","#     model.add(Dense(64, activation='relu'))\n","#     model.add(Dense(64, activation='relu'))\n","#     model.add(Dense(32, activation='relu'))\n","#     model.add(Dropout(0.5))\n","#     model.add(Flatten())\n","#     model.add(Dense(y_shape, activation='softmax'))\n","#     # model.add(Reshape((y_shape,)))\n","#     model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['categorical_accuracy'])\n","#     return model\n","\n","# model = create_model(embeddings, x_train.shape[1], y_train.shape[1])\n","# model.summary()"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[],"source":["def createModel_tuner(hp):\n","    embeddings = (5157, 50)\n","    x_shape = 36\n","    y_shape = 3\n","    model = Sequential()\n","    model.add(Input(shape=(x_shape,)))\n","    model.add(Embedding(input_dim=embeddings[0], output_dim=embeddings[1], trainable=False))\n","    for i in range(hp.Int('num_layers', 2, 10)):\n","        model.add(Dense(units=hp.Int('units_' + str(i),\n","                                        min_value=32,\n","                                        max_value=256,\n","                                        step=32),\n","                            activation='relu'))\n","    model.add(Dropout(hp.Float('dropout', 0, 0.8, step=0.1, default=0.5)))\n","    model.add(Flatten())\n","    model.add(Dense(y_shape, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['categorical_accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Search space summary\n","Default search space size: 4\n","num_layers (Int)\n","{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 10, 'step': 1, 'sampling': 'linear'}\n","units_0 (Int)\n","{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n","units_1 (Int)\n","{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n","dropout (Float)\n","{'default': 0.5, 'conditions': [], 'min_value': 0.0, 'max_value': 0.8, 'step': 0.1, 'sampling': 'linear'}\n"]}],"source":["tuner = RandomSearch(\n","    createModel_tuner,\n","    objective='categorical_accuracy',\n","    max_trials=10,\n","    directory='tunerDir',\n","    project_name='tuner')\n","\n","tuner.search_space_summary()"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 10 Complete [00h 19m 47s]\n","categorical_accuracy: 0.9711975455284119\n","\n","Best categorical_accuracy So Far: 0.9857150316238403\n","Total elapsed time: 02h 37m 20s\n","INFO:tensorflow:Oracle triggered exit\n"]}],"source":["tunerHistory = tuner.search(x_train, y_train, epochs=70, batch_size=32, validation_split=0.2)"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Results summary\n","Results in tunerDir\\tuner\n","Showing 5 best trials\n","Objective(name=\"categorical_accuracy\", direction=\"max\")\n","\n","Trial 06 summary\n","Hyperparameters:\n","num_layers: 6\n","units_0: 64\n","units_1: 192\n","dropout: 0.0\n","units_2: 256\n","units_3: 32\n","units_4: 224\n","units_5: 160\n","units_6: 192\n","units_7: 160\n","Score: 0.9857150316238403\n","\n","Trial 08 summary\n","Hyperparameters:\n","num_layers: 4\n","units_0: 96\n","units_1: 224\n","dropout: 0.30000000000000004\n","units_2: 224\n","units_3: 96\n","units_4: 160\n","units_5: 192\n","units_6: 64\n","units_7: 160\n","units_8: 160\n","units_9: 32\n","Score: 0.9750723242759705\n","\n","Trial 09 summary\n","Hyperparameters:\n","num_layers: 8\n","units_0: 256\n","units_1: 192\n","dropout: 0.1\n","units_2: 32\n","units_3: 160\n","units_4: 32\n","units_5: 256\n","units_6: 64\n","units_7: 256\n","units_8: 128\n","units_9: 32\n","Score: 0.9711975455284119\n","\n","Trial 01 summary\n","Hyperparameters:\n","num_layers: 8\n","units_0: 192\n","units_1: 32\n","dropout: 0.0\n","units_2: 32\n","units_3: 32\n","units_4: 32\n","units_5: 32\n","units_6: 32\n","units_7: 32\n","Score: 0.9672194719314575\n","\n","Trial 05 summary\n","Hyperparameters:\n","num_layers: 7\n","units_0: 64\n","units_1: 32\n","dropout: 0.0\n","units_2: 256\n","units_3: 256\n","units_4: 224\n","units_5: 160\n","units_6: 32\n","units_7: 128\n","Score: 0.9621564149856567\n"]}],"source":["tuner.results_summary(num_trials=5)"]},{"cell_type":"code","execution_count":199,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 36, 50)            257850    \n","                                                                 \n"," dense (Dense)               (None, 36, 64)            3264      \n","                                                                 \n"," dense_1 (Dense)             (None, 36, 192)           12480     \n","                                                                 \n"," dense_2 (Dense)             (None, 36, 256)           49408     \n","                                                                 \n"," dense_3 (Dense)             (None, 36, 32)            8224      \n","                                                                 \n"," dense_4 (Dense)             (None, 36, 224)           7392      \n","                                                                 \n"," dense_5 (Dense)             (None, 36, 160)           36000     \n","                                                                 \n"," dropout (Dropout)           (None, 36, 160)           0         \n","                                                                 \n"," flatten (Flatten)           (None, 5760)              0         \n","                                                                 \n"," dense_6 (Dense)             (None, 3)                 17283     \n","                                                                 \n","=================================================================\n","Total params: 391,901\n","Trainable params: 134,051\n","Non-trainable params: 257,850\n","_________________________________________________________________\n"]}],"source":["# tunedModel = tuner.get_best_models(num_models=1)[0]\n","# tunedModel.save('hw3_2_best_model')\n","\n","model = load_model('hw3_2_best_model')\n","model.summary()"]},{"cell_type":"code","execution_count":200,"metadata":{"id":"iMjs0W69y_nb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.1308 - categorical_accuracy: 0.9631\n","Epoch 2/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.1008 - categorical_accuracy: 0.9665\n","Epoch 3/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0923 - categorical_accuracy: 0.9693\n","Epoch 4/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0869 - categorical_accuracy: 0.9699\n","Epoch 5/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0821 - categorical_accuracy: 0.9718\n","Epoch 6/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0803 - categorical_accuracy: 0.9712\n","Epoch 7/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0729 - categorical_accuracy: 0.9743\n","Epoch 8/70\n","1513/1513 [==============================] - 13s 9ms/step - loss: 0.0708 - categorical_accuracy: 0.9751\n","Epoch 9/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0688 - categorical_accuracy: 0.9752\n","Epoch 10/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0639 - categorical_accuracy: 0.9764\n","Epoch 11/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0643 - categorical_accuracy: 0.9777\n","Epoch 12/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0633 - categorical_accuracy: 0.9771\n","Epoch 13/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0575 - categorical_accuracy: 0.9794\n","Epoch 14/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0592 - categorical_accuracy: 0.9797\n","Epoch 15/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0566 - categorical_accuracy: 0.9799\n","Epoch 16/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0539 - categorical_accuracy: 0.9816\n","Epoch 17/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0566 - categorical_accuracy: 0.9797\n","Epoch 18/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0532 - categorical_accuracy: 0.9808\n","Epoch 19/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0516 - categorical_accuracy: 0.9814\n","Epoch 20/70\n","1513/1513 [==============================] - 16s 10ms/step - loss: 0.0507 - categorical_accuracy: 0.9822\n","Epoch 21/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0486 - categorical_accuracy: 0.9830\n","Epoch 22/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0483 - categorical_accuracy: 0.9829\n","Epoch 23/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0483 - categorical_accuracy: 0.9831\n","Epoch 24/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0451 - categorical_accuracy: 0.9844\n","Epoch 25/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0437 - categorical_accuracy: 0.9839\n","Epoch 26/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0473 - categorical_accuracy: 0.9834\n","Epoch 27/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0440 - categorical_accuracy: 0.9843\n","Epoch 28/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0422 - categorical_accuracy: 0.9848\n","Epoch 29/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0429 - categorical_accuracy: 0.9854\n","Epoch 30/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0418 - categorical_accuracy: 0.9856\n","Epoch 31/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0423 - categorical_accuracy: 0.9852\n","Epoch 32/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0394 - categorical_accuracy: 0.9860\n","Epoch 33/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0404 - categorical_accuracy: 0.9859\n","Epoch 34/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0400 - categorical_accuracy: 0.9860\n","Epoch 35/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0398 - categorical_accuracy: 0.9861\n","Epoch 36/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0404 - categorical_accuracy: 0.9863\n","Epoch 37/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0410 - categorical_accuracy: 0.9860\n","Epoch 38/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0343 - categorical_accuracy: 0.9878\n","Epoch 39/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0388 - categorical_accuracy: 0.9865\n","Epoch 40/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0398 - categorical_accuracy: 0.9862\n","Epoch 41/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0377 - categorical_accuracy: 0.9862\n","Epoch 42/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0377 - categorical_accuracy: 0.9869\n","Epoch 43/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0374 - categorical_accuracy: 0.9869\n","Epoch 44/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0333 - categorical_accuracy: 0.9883\n","Epoch 45/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0358 - categorical_accuracy: 0.9874\n","Epoch 46/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0372 - categorical_accuracy: 0.9871\n","Epoch 47/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0362 - categorical_accuracy: 0.9875\n","Epoch 48/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0342 - categorical_accuracy: 0.9876\n","Epoch 49/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0371 - categorical_accuracy: 0.9875\n","Epoch 50/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0326 - categorical_accuracy: 0.9884\n","Epoch 51/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0342 - categorical_accuracy: 0.9887\n","Epoch 52/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0361 - categorical_accuracy: 0.9873\n","Epoch 53/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0316 - categorical_accuracy: 0.9889\n","Epoch 54/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0349 - categorical_accuracy: 0.9880\n","Epoch 55/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0335 - categorical_accuracy: 0.9883\n","Epoch 56/70\n","1513/1513 [==============================] - 15s 10ms/step - loss: 0.0320 - categorical_accuracy: 0.9892\n","Epoch 57/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0358 - categorical_accuracy: 0.9878\n","Epoch 58/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0315 - categorical_accuracy: 0.9893\n","Epoch 59/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0329 - categorical_accuracy: 0.9889\n","Epoch 60/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0337 - categorical_accuracy: 0.9887\n","Epoch 61/70\n","1513/1513 [==============================] - 14s 10ms/step - loss: 0.0323 - categorical_accuracy: 0.9886\n","Epoch 62/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0326 - categorical_accuracy: 0.9885\n","Epoch 63/70\n","1513/1513 [==============================] - 13s 9ms/step - loss: 0.0338 - categorical_accuracy: 0.9883\n","Epoch 64/70\n","1513/1513 [==============================] - 13s 9ms/step - loss: 0.0300 - categorical_accuracy: 0.9897\n","Epoch 65/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0347 - categorical_accuracy: 0.9883\n","Epoch 66/70\n","1513/1513 [==============================] - 13s 9ms/step - loss: 0.0316 - categorical_accuracy: 0.9892\n","Epoch 67/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0322 - categorical_accuracy: 0.9893\n","Epoch 68/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0313 - categorical_accuracy: 0.9898\n","Epoch 69/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0347 - categorical_accuracy: 0.9877\n","Epoch 70/70\n","1513/1513 [==============================] - 14s 9ms/step - loss: 0.0320 - categorical_accuracy: 0.9887\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1d525858730>"]},"execution_count":200,"metadata":{},"output_type":"execute_result"}],"source":["# Write your code here\n","model.fit(x_train, y_train, epochs=70, batch_size=32, use_multiprocessing=True)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
