{"cells":[{"cell_type":"markdown","metadata":{"id":"Y1cDcKRZwXCL"},"source":["# Key-Value Attention Mechanism Homework on Keras: Character-level Machine Translation (Many-to-Many, encoder-decoder)\n","\n","In this homework, you will create an MT model with key-value attention mechnism that coverts names of constituency MP candidates in the 2019 Thai general election from Thai script to Roman(Latin) script. E.g. นิยม-->niyom "]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6024,"status":"ok","timestamp":1680718678537,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Oy6QYsP4wa-k","outputId":"8ac7d294-ec86-484c-d1cb-af31f6665b9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.11.0\n"]}],"source":["# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","import matplotlib as mpl\n","import matplotlib.font_manager as fm\n","\n","fm.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n","mpl.rc('font', family='TH Sarabun New')"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import keras\n","import numpy as np\n","import random\n","np.random.seed(0)\n","\n","from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from keras.layers import RepeatVector, Dense, Activation, Lambda\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical, pad_sequences\n","from keras.models import load_model, Model\n","from keras import backend as K\n","\n","# %matplotlib inline\n","# from tensorflow.keras.preprocessing.sequence import pad_sequences\n","# from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","# from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras.utils import to_categorical\n","# from tensorflow.keras import Model\n","# from tensorflow.keras.models import load_model\n","# import tensorflow.keras.backend as K\n","# import numpy as np\n","\n","# import random"]},{"cell_type":"markdown","metadata":{"id":"Sq20keO6wXCh"},"source":["## Load Dataset\n","We have generated a toy dataset using names of constituency MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n","\n","<img src=\"https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/images/dataset_diagram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78675,"status":"ok","timestamp":1680719078872,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"B8ILK0shprTa","outputId":"55715c6f-dfe1-4bf5-ad12-a021e5f8018a"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# import shutil\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/mp_name_th_en.csv\", \"/content/mp_name_th_en.csv\")"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"rTQk8W4OwXCk"},"outputs":[],"source":["import csv\n","with open('data/mp_name_th_en.csv') as csvfile:\n","    readCSV = csv.reader(csvfile, delimiter=',')\n","    name_th = []\n","    name_en = []\n","    for row in readCSV:\n","        name_th.append(row[0])\n","        name_en.append(row[1])"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680719134852,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"lVNHVM_FwXCs","outputId":"1b5c8a4d-5d04-4266-e734-5e199bedaf05"},"outputs":[{"name":"stdout","output_type":"stream","text":["ไกรสีห์ kraisi\n","พัชรี phatri\n","ธีระ thira\n","วุฒิกร wutthikon\n","ไสว sawai\n","สัมภาษณ์  samphat\n","วศิน wasin\n","ทินวัฒน์ thinwat\n","ศักดินัย sakdinai\n","สุรศักดิ์ surasak\n"]}],"source":["for th, en in zip(name_th[:10],name_en[:10]):\n","    print(th,en)"]},{"cell_type":"markdown","metadata":{"id":"heMTiM7qwXC2"},"source":["## Task1: Preprocess dataset for Keras\n","* 2 dictionaries for indexing (1 for input and another for output)\n","* DON'T FORGET TO INCLUDE special token for padding\n","* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)\n","* Be mindful of your pad_sequences \"padding\" hyperparameter. Choose wisely (post-padding vs pre-padding)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"_O5YhjntwXC4"},"outputs":[{"name":"stdout","output_type":"stream","text":["data has 10887 names, 65 unique characters in input, 24 unique characters in output.\n","max input length is 20\n"]}],"source":["#FILL YOUR CODE HERE\n","\n","input_chars = list(set(''.join(name_th)))\n","output_chars = list(set(''.join(name_en)))\n","\n","data_size, vocab_size = len(name_th), len(input_chars)+1 # 1 for padding\n","output_size, output_vocab_size = len(name_en), len(output_chars)+ 2 # 1 for padding 1 for end of word\n","\n","print('data has %d names, %d unique characters in input, %d unique characters in output.' % (data_size, vocab_size, output_vocab_size))\n","maxlen = len( max(name_th, key=len)) #max input length\n","print('max input length is', maxlen)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input_char_indices {0: '<PAD>', 1: ' ', 2: 'ก', 3: 'ข', 4: 'ค', 5: 'ฆ', 6: 'ง', 7: 'จ', 8: 'ฉ', 9: 'ช', 10: 'ซ', 11: 'ฌ', 12: 'ญ', 13: 'ฎ', 14: 'ฏ', 15: 'ฐ', 16: 'ฑ', 17: 'ฒ', 18: 'ณ', 19: 'ด', 20: 'ต', 21: 'ถ', 22: 'ท', 23: 'ธ', 24: 'น', 25: 'บ', 26: 'ป', 27: 'ผ', 28: 'ฝ', 29: 'พ', 30: 'ฟ', 31: 'ภ', 32: 'ม', 33: 'ย', 34: 'ร', 35: 'ล', 36: 'ว', 37: 'ศ', 38: 'ษ', 39: 'ส', 40: 'ห', 41: 'ฬ', 42: 'อ', 43: 'ฮ', 44: 'ะ', 45: 'ั', 46: 'า', 47: 'ำ', 48: 'ิ', 49: 'ี', 50: 'ึ', 51: 'ื', 52: 'ุ', 53: 'ู', 54: 'เ', 55: 'แ', 56: 'โ', 57: 'ใ', 58: 'ไ', 59: '็', 60: '่', 61: '้', 62: '๊', 63: '๋', 64: '์'}\n","output_char_indices {0: '<PAD>', 1: '<EOS>', 2: '-', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'w', 23: 'y'}\n"]}],"source":["sorted_input_chars = sorted(input_chars)\n","sorted_output_chars = sorted(output_chars)\n","\n","sorted_input_chars.insert(0,\"<PAD>\") #PADDING\n","sorted_output_chars.insert(0,\"<EOS>\") #END OF WORD\n","sorted_output_chars.insert(0,\"<PAD>\") #PADDING\n","\n","# Input dictionary\n","input_char2idx = dict((c, i) for i, c in enumerate(sorted_input_chars))\n","input_idx2char = dict((i, c) for i, c in enumerate(sorted_input_chars))\n","# Output dictionary\n","output_char2idx = dict((c, i) for i, c in enumerate(sorted_output_chars))\n","output_idx2char = dict((i, c) for i, c in enumerate(sorted_output_chars))\n","\n","print('input_char_indices', input_idx2char)\n","print('output_char_indices', output_idx2char)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["samplesize 10887\n","max input length is 20\n","max output length is 40\n"]}],"source":["print('samplesize', data_size)\n","Tx = maxlen\n","# Ty = len(max(name_en, key=len)) # max output length\n","Ty = Tx * 2\n","print('max input length is', Tx)\n","print('max output length is', Ty)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Befor padding:\n","X [[58, 2, 34, 39, 49, 40, 64], [29, 45, 9, 34, 49], [23, 49, 34, 44]]\n","Y [[12, 18, 3, 11, 19, 11], [17, 10, 3, 20, 18, 11], [20, 10, 11, 18, 3]]\n","After padding:\n","X [[58  2 34 39 49 40 64  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [29 45  9 34 49  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [23 49 34 44  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n","Y [[12 18  3 11 19 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [17 10  3 20 18 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [20 10 11 18  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"]}],"source":["# Padding\n","X = []\n","for name in name_th:\n","    temp = []\n","    for char in name:\n","        temp.append(input_char2idx[char])\n","    X.append(temp)\n","\n","Y = []\n","for name in name_en:\n","    temp = []\n","    for char in name:\n","        temp.append(output_char2idx[char])\n","    Y.append(temp)\n","\n","print(f'Befor padding:')\n","print('X', X[:3])\n","print('Y', Y[:3])\n","\n","# We choose padding='post' because we want to pad after the sentence\n","X = pad_sequences(X, maxlen=Tx, padding='post', value=0)\n","Y = pad_sequences(Y, maxlen=Ty, padding='post', value=0)\n","\n","print(f'After padding:')\n","print('X', X[:3])\n","print('Y', Y[:3])"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape: (10887, 20, 65)\n","Y shape: (10887, 40, 24)\n"]}],"source":["# One-hot encoding\n","X = to_categorical(X, num_classes=vocab_size)\n","Y = to_categorical(Y, num_classes=output_vocab_size)\n","\n","X = X.reshape(data_size, Tx, vocab_size)\n","Y = Y.reshape(data_size, Ty, output_vocab_size)\n","\n","print(f'X shape: {X.shape}')\n","print(f'Y shape: {Y.shape}')"]},{"cell_type":"markdown","metadata":{"id":"HNqqnkVSwXC-"},"source":["# Attention Mechanism\n","## Task 2: Code your own (key-value) attention mechnism\n","* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n","* Define global variables\n","* fill code for one_step_attention function\n","* Hint: use keras.layers.Lambda \n","* Hint: you will probably need more hidden dimmensions than what you've seen in the demo\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSdFcuGuwXDB"},"outputs":[],"source":["from keras.activations import softmax\n","from keras.layers import Lambda\n","\n","def softMaxAxis1(x):\n","    return softmax(x,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BS3Ziti1wXDH"},"outputs":[],"source":["#These are global variables (shared layers)\n","## Fill your code here\n","## you are allowed to use code in the demo as your template. \n","\n","#repeater = ???\n","#concatenator = ???\n","repeator = RepeatVector(Tx)\n","concatenator = Concatenate(axis=-1)\n","\n","#Key-values (Hint)\n","splitter = Lambda(lambda x:tf.split(x, num_or_size_splits=2,aixs=2)) \n","\n","#fatten_1 = ???\n","#fatten_2 = ???\n","fatten_1 = Dense(1, activation = \"tanh\")\n","fatten_2 = Dense(1, activation = \"relu\")\n","\n","#activator = ???\n","activator = Activation(softMaxAxis1, name='attention_score') # We are using a custom softmax(axis = 1) loaded in this notebook\n","#dotor = ???\n","dotor = Dot(axes = 1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecNci8x5wXDN"},"outputs":[],"source":["def one_step_attention(a, s_prev):\n","\n","    #Fill code here\n","    #key, value = ???\n","\n","\n","    #concat = ...key...\n","    #context = ...value...\n","\n","    # Repeat the decoder hidden state to concat with encoder hidden states\n","    s_prev = repeator(s_prev)\n","    concat = concatenator([a,s_prev])\n","    # attention function\n","    e = fattn_1(concat)\n","    energies =fattn_2(e)\n","    # calculate attention_scores (softmax)\n","    attention_scores = activator(energies)\n","    #calculate a context vector\n","    context = dotor([attention_scores,a])\n","\n","    return None # return whatever you need to complete this homework "]},{"cell_type":"markdown","metadata":{"id":"2bgSCY3NwXDU"},"source":["## Task3: Create and train your encoder/decoder model here\n","* HINT: you will probably need more hidden dimmensions than what you've seen in the demo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CVrgh9nwXDV"},"outputs":[],"source":["#FILL CODE HERE :Hint --> heatmap in CNN + GradCAM\n","\n","# def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#   ...\n","\n","# def inference_encoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#    X = ....\n","#    h = ....\n","\n","#    model = Model(inputs=[X],outputs=h)\n","#    return model\n","\n","# def inference_decoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#   s0 = ...\n","#   c0 = ...\n","#   h = ...\n","#   context, attention_scores, energies = one_step_attention(h, s)\n","#   ...decoder_LSTM_cell...\n","#   out = output_layer(s)\n","\n","#   model = Model(inputs=[h,s0,c0],outputs=[out,s,c,atten_score,energies])\n","\n","#   return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XptuOQj-wXDb"},"outputs":[],"source":["#FIT YOUR MODEL HERE"]},{"cell_type":"markdown","metadata":{"id":"3C2RET9GwXDh"},"source":["# Thai-Script to Roman-Script Translation\n","* Task 4: Test your model on 5 examples of your choice including your name! \n","* Task 5: Show your visualization of attention scores on one of your example "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gON7T2xVwXDk"},"outputs":[],"source":["#task 4\n","#fill your code here\n"]},{"cell_type":"markdown","metadata":{"id":"r9-mxbsKwXDp"},"source":["### Plot the attention map\n","* If you need to install thai font: sudo apt install xfonts-thai\n","* this is what your visualization might look like:\n","--> https://drive.google.com/file/d/168J5SPSf4NNKj718wWUEDpUbh8QYZKux/view?usp=share_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4lhl4Vsz6Y8"},"outputs":[],"source":["# EXAMPLES = ???\n","# h = inferEncoder_model.predict(EXAMPLES)\n","# s0 = ???\n","# c0 = ???\n","# ...\n","# Ty = 10\n","# for t in range(Ty):\n","#   out,s,c,attention_scores,energies = inferDecoder_model.predict([h,s0,c0])\n","# ...\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRL8hHaLwXDq"},"outputs":[],"source":["#task 5\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.rcParams['font.family']='TH Sarabun New'  #you can change to other font that works for you\n","#fill your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XT44RFuxqzBM"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
