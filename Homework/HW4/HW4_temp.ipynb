{"cells":[{"cell_type":"markdown","metadata":{"id":"Y1cDcKRZwXCL"},"source":["# Key-Value Attention Mechanism Homework on Keras: Character-level Machine Translation (Many-to-Many, encoder-decoder)\n","\n","In this homework, you will create an MT model with key-value attention mechnism that coverts names of constituency MP candidates in the 2019 Thai general election from Thai script to Roman(Latin) script. E.g. นิยม-->niyom "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6024,"status":"ok","timestamp":1680718678537,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Oy6QYsP4wa-k","outputId":"8ac7d294-ec86-484c-d1cb-af31f6665b9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.10.1\n"]}],"source":["# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","import matplotlib as mpl\n","import matplotlib.font_manager as fm\n","\n","fm.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n","mpl.rc('font', family='TH Sarabun New')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import keras\n","import numpy as np\n","import random\n","np.random.seed(0)\n","\n","from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from keras.layers import RepeatVector, Dense, Activation, Lambda\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical, pad_sequences\n","from keras.models import load_model, Model\n","from keras import backend as K\n","\n","# %matplotlib inline\n","# from tensorflow.keras.preprocessing.sequence import pad_sequences\n","# from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","# from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras.utils import to_categorical\n","# from tensorflow.keras import Model\n","# from tensorflow.keras.models import load_model\n","# import tensorflow.keras.backend as K\n","# import numpy as np\n","\n","# import random"]},{"cell_type":"markdown","metadata":{"id":"Sq20keO6wXCh"},"source":["## Load Dataset\n","We have generated a toy dataset using names of constituency MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n","\n","<img src=\"https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/images/dataset_diagram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78675,"status":"ok","timestamp":1680719078872,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"B8ILK0shprTa","outputId":"55715c6f-dfe1-4bf5-ad12-a021e5f8018a"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# import shutil\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/mp_name_th_en.csv\", \"/content/mp_name_th_en.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rTQk8W4OwXCk"},"outputs":[],"source":["import csv\n","with open('data/mp_name_th_en.csv') as csvfile:\n","    readCSV = csv.reader(csvfile, delimiter=',')\n","    name_th = []\n","    name_en = []\n","    for row in readCSV:\n","        name_th.append(row[0])\n","        name_en.append(row[1])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680719134852,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"lVNHVM_FwXCs","outputId":"1b5c8a4d-5d04-4266-e734-5e199bedaf05"},"outputs":[{"name":"stdout","output_type":"stream","text":["ไกรสีห์ kraisi\n","พัชรี phatri\n","ธีระ thira\n","วุฒิกร wutthikon\n","ไสว sawai\n","สัมภาษณ์  samphat\n","วศิน wasin\n","ทินวัฒน์ thinwat\n","ศักดินัย sakdinai\n","สุรศักดิ์ surasak\n"]}],"source":["for th, en in zip(name_th[:10],name_en[:10]):\n","    print(th,en)"]},{"cell_type":"markdown","metadata":{"id":"heMTiM7qwXC2"},"source":["## Task1: Preprocess dataset for Keras\n","* 2 dictionaries for indexing (1 for input and another for output)\n","* DON'T FORGET TO INCLUDE special token for padding\n","* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)\n","* Be mindful of your pad_sequences \"padding\" hyperparameter. Choose wisely (post-padding vs pre-padding)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_O5YhjntwXC4"},"outputs":[{"name":"stdout","output_type":"stream","text":["data has 10887 names, 65 unique characters in input, 24 unique characters in output.\n","max input length is 20\n"]}],"source":["#FILL YOUR CODE HERE\n","\n","input_chars = list(set(''.join(name_th)))\n","output_chars = list(set(''.join(name_en)))\n","\n","data_size, vocab_size = len(name_th), len(input_chars)+1 # 1 for padding\n","output_size, output_vocab_size = len(name_en), len(output_chars)+ 2 # 1 for padding 1 for end of word\n","\n","print('data has %d names, %d unique characters in input, %d unique characters in output.' % (data_size, vocab_size, output_vocab_size))\n","maxlen = len( max(name_th, key=len)) #max input length\n","print('max input length is', maxlen)\n","\n","m = data_size"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input_char_indices {0: '<PAD>', 1: ' ', 2: 'ก', 3: 'ข', 4: 'ค', 5: 'ฆ', 6: 'ง', 7: 'จ', 8: 'ฉ', 9: 'ช', 10: 'ซ', 11: 'ฌ', 12: 'ญ', 13: 'ฎ', 14: 'ฏ', 15: 'ฐ', 16: 'ฑ', 17: 'ฒ', 18: 'ณ', 19: 'ด', 20: 'ต', 21: 'ถ', 22: 'ท', 23: 'ธ', 24: 'น', 25: 'บ', 26: 'ป', 27: 'ผ', 28: 'ฝ', 29: 'พ', 30: 'ฟ', 31: 'ภ', 32: 'ม', 33: 'ย', 34: 'ร', 35: 'ล', 36: 'ว', 37: 'ศ', 38: 'ษ', 39: 'ส', 40: 'ห', 41: 'ฬ', 42: 'อ', 43: 'ฮ', 44: 'ะ', 45: 'ั', 46: 'า', 47: 'ำ', 48: 'ิ', 49: 'ี', 50: 'ึ', 51: 'ื', 52: 'ุ', 53: 'ู', 54: 'เ', 55: 'แ', 56: 'โ', 57: 'ใ', 58: 'ไ', 59: '็', 60: '่', 61: '้', 62: '๊', 63: '๋', 64: '์'}\n","output_char_indices {0: '<PAD>', 1: '<EOS>', 2: '-', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'w', 23: 'y'}\n"]}],"source":["sorted_input_chars = sorted(input_chars)\n","sorted_output_chars = sorted(output_chars)\n","\n","sorted_input_chars.insert(0,\"<PAD>\") #PADDING\n","sorted_output_chars.insert(0,\"<EOS>\") #END OF WORD\n","sorted_output_chars.insert(0,\"<PAD>\") #PADDING\n","\n","# Input dictionary\n","input_char2idx = dict((c, i) for i, c in enumerate(sorted_input_chars))\n","input_idx2char = dict((i, c) for i, c in enumerate(sorted_input_chars))\n","# Output dictionary\n","output_char2idx = dict((c, i) for i, c in enumerate(sorted_output_chars))\n","output_idx2char = dict((i, c) for i, c in enumerate(sorted_output_chars))\n","\n","print('input_char_indices', input_idx2char)\n","print('output_char_indices', output_idx2char)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["samplesize 10887\n","max input length is 20\n","max output length is 40\n"]}],"source":["print('samplesize', data_size)\n","Tx = maxlen\n","# Ty = len(max(name_en, key=len)) # max output length\n","Ty = Tx * 2\n","print('max input length is', Tx)\n","print('max output length is', Ty)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Befor padding:\n","Sample X: [58, 2, 34, 39, 49, 40, 64] -> ไกรสีห์\n","Sample Y: [12, 18, 3, 11, 19, 11] -> kraisi\n","After padding:\n","Sample X: [58  2 34 39 49 40 64  0  0  0  0  0  0  0  0  0  0  0  0  0] -> ไกรสีห์<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n","Sample Y: [12 18  3 11 19 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] -> kraisi<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"]}],"source":["# Padding\n","X = []\n","for name in name_th:\n","    temp = []\n","    for char in name:\n","        temp.append(input_char2idx[char])\n","    X.append(temp)\n","\n","Y = []\n","for name in name_en:\n","    temp = []\n","    for char in name:\n","        temp.append(output_char2idx[char])\n","    Y.append(temp)\n","\n","print(f'Befor padding:')\n","xs = ''.join([input_idx2char[i] for i in X[0]])\n","print(f'Sample X: {X[0]} -> {xs}')\n","ys = ''.join([output_idx2char[i] for i in Y[0]])\n","print(f'Sample Y: {Y[0]} -> {ys}')\n","\n","# We choose padding='post' because we want to pad after the sentence\n","X = pad_sequences(X, maxlen=Tx, padding='post', value=0)\n","Y = pad_sequences(Y, maxlen=Ty, padding='post', value=0)\n","\n","print(f'After padding:')\n","xs = ''.join([input_idx2char[i] for i in X[0]])\n","print(f'Sample X: {X[0]} -> {xs}')\n","ys = ''.join([output_idx2char[i] for i in Y[0]]) \n","print(f'Sample Y: {Y[0]} -> {ys}')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape: (10887, 20, 65)\n","Y shape: (10887, 40, 24)\n"]}],"source":["# One-hot encoding\n","X = to_categorical(X, num_classes=vocab_size)\n","Y = to_categorical(Y, num_classes=output_vocab_size)\n","\n","X = X.reshape(data_size, Tx, vocab_size)\n","Y = Y.reshape(data_size, Ty, output_vocab_size)\n","\n","print(f'X shape: {X.shape}')\n","print(f'Y shape: {Y.shape}')"]},{"cell_type":"markdown","metadata":{"id":"HNqqnkVSwXC-"},"source":["# Attention Mechanism\n","## Task 2: Code your own (key-value) attention mechnism\n","* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n","* Define global variables\n","* fill code for one_step_attention function\n","* Hint: use keras.layers.Lambda \n","* Hint: you will probably need more hidden dimmensions than what you've seen in the demo\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"eSdFcuGuwXDB"},"outputs":[],"source":["from keras.activations import softmax\n","from keras.layers import Lambda\n","\n","def softMaxAxis1(x):\n","    return softmax(x,axis=1)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"BS3Ziti1wXDH"},"outputs":[],"source":["#These are global variables (shared layers)\n","## Fill your code here\n","## you are allowed to use code in the demo as your template. \n","\n","#repeater = ???\n","#concatenator = ???\n","repeator = RepeatVector(Tx, name='repeatorA')\n","concatenator = Concatenate(axis=-1, name='concatenatorA')\n","\n","#Key-values (Hint)\n","splitter = Lambda(lambda x:tf.split(x, num_or_size_splits=2,axis=2), name='splitterA') \n","\n","#fatten_1 = ???\n","#fatten_2 = ???\n","fatten_1 = Dense(1, activation = \"tanh\", name = \"fatten_1A\")\n","fatten_2 = Dense(1, activation = \"relu\", name = \"fatten_2A\")\n","\n","#activator = ???\n","activator = Activation(softMaxAxis1, name='attention_scoreA') # We are using a custom softmax(axis = 1) loaded in this notebook\n","#dotor = ???\n","dotor = Dot(axes = 1, name='dotorA')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ecNci8x5wXDN"},"outputs":[],"source":["def one_step_attention(a, s_prev):\n","\n","    # #Fill code here\n","    # #key, value = ???\n","    # key, value = splitter(a)\n","    # #concat = ...key...\n","    # # Repeat the decoder hidden state to concat with encoder hidden states\n","    # s_prev = repeator(s_prev)\n","    # concat = concatenator([key, s_prev])\n","    # # Attention function\n","    # e = fatten_1(concat)\n","    # energies = fatten_2(e)\n","    # # Calculate attention weights\n","    # alphas = attention_scores = activator(energies)\n","    # #context = ...value...\n","    # context = dotor([alphas, value])\n","\n","    # From Equation in the slides a = h (hidden state)(input), s_prev = s_{t-1} (Previous hidden state)\n","    # Get Key and Value from a (h) = [key, value]\n","    [key, value] = splitter(a)\n","\n","    # Find M and pass it through a tanh layer\n","    # M is the concatenation of the previous hidden state and the current hidden state\n","    s_prev = repeator(s_prev)\n","    concat = concatenator([key, s_prev])\n","    e = fatten_1(concat)\n","\n","    # Find energies and pass it through a relu layer\n","    energies = fatten_2(e)\n","\n","    # Calculate attention scores through softmax\n","    attention_scores = activator(energies)\n","\n","    # Find context vector\n","    context = dotor([attention_scores, value])\n"," \n","    return context, attention_scores , energies # return whatever you need to complete this homework "]},{"cell_type":"markdown","metadata":{"id":"2bgSCY3NwXDU"},"source":["## Task3: Create and train your encoder/decoder model here\n","* HINT: you will probably need more hidden dimmensions than what you've seen in the demo"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3CVrgh9nwXDV"},"outputs":[],"source":["#FILL CODE HERE :Hint --> heatmap in CNN + GradCAM\n","\n","# def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#   ...\n","\n","# def inference_encoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#    X = ....\n","#    h = ....\n","\n","#    model = Model(inputs=[X],outputs=h)\n","#    return model\n","\n","# def inference_decoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#   s0 = ...\n","#   c0 = ...\n","#   h = ...\n","#   context, attention_scores, energies = one_step_attention(h, s)\n","#   ...decoder_LSTM_cell...\n","#   out = output_layer(s)\n","\n","#   model = Model(inputs=[h,s0,c0],outputs=[out,s,c,atten_score,energies])\n","\n","#   return model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["n_h = 32 #hidden dimensions for encoder \n","n_s = 64 #hidden dimensions for decoder\n","encoder_LSTM =  Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(-1, Tx, n_h*2), name = 'encoder_LSTM') #encoder_LSTM\n","decoder_LSTM_cell = LSTM(n_s, return_state = True, name='decoder_LSTM_cell') #decoder_LSTM_cell\n","output_layer = Dense(output_vocab_size, activation=\"softmax\", name='output_layerA') #output_layer"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","    \"\"\"\n","    Arguments:\n","    Tx -- length of the input sequence\n","    Ty -- length of the output sequence\n","    n_h -- hidden state size of the Bi-LSTM\n","    n_s -- hidden state size of the post-attention LSTM\n","    vocab_size -- size of the input vocab\n","    output_vocab_size -- size of the output vocab\n","\n","    Returns:\n","    model -- Keras model instance\n","    \"\"\"\n","    # Define the input of your model\n","    X = Input(shape=(Tx, vocab_size), name='Encoder_Input')\n","    # Define hidden state and cell state for decoder_LSTM_Cell\n","    s0 = Input(shape=(n_s,), name='s0')\n","    c0 = Input(shape=(n_s,), name='c0')\n","    s = s0\n","    c = c0\n","    \n","    # Initialize empty list of outputs\n","    outputs = list()\n","    \n","    # Encoder Bi-LSTM\n","    h = encoder_LSTM(X)\n","    \n","    # Iterate for Ty steps\n","    for t in range(Ty):\n","        # Perform one step of the attention mechanism to get back the context vector at step t\n","        context, attention_scores, energies = one_step_attention(h, s)\n","        \n","        # Apply the post-attention LSTM cell to the \"context\" vector.\n","        s, _, c = decoder_LSTM_cell(context, initial_state=[s, c])\n","        \n","        # Apply Dense layer to the hidden state output of the post-attention LSTM\n","        out = output_layer(s)\n","        \n","        # Append \"out\" to the \"outputs\" list\n","        outputs.append(out)\n","    \n","    # Create model instance\n","    model = Model(inputs=[X, s0, c0], outputs=outputs)\n","    \n","    return model"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"XptuOQj-wXDb"},"outputs":[],"source":["#FIT YOUR MODEL HERE"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","    \n","#     # Initialize empty list of outputs\n","#     outputs = list()\n","    \n","#     # Set up encoder \n","#     encoder_input = Input(shape=(Tx, vocab_size))\n","#     encoderLSTM = Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(-1, Tx, n_h*2))\n","#     # Encoder output 1. encoder_output, 2. encoder_hidden_state, 3. encoder_cell_state\n","#     encoder_output, state_h, state_c = encoderLSTM(encoder_input)\n","    \n","#     # Define encoder states\n","#     encoder_states = [state_h, state_c]\n","    \n","#     # Set up decoder\n","\n","#     # Define decoder initial states\n","#     decoder_state_input_h = Input(shape=(n_s,), name='s0')\n","#     decoder_state_input_c = Input(shape=(n_s,), name='c0')\n","#     state_s = decoder_state_input_h\n","#     state_c = decoder_state_input_c\n","    \n","#     # Set up decoder LSTM\n","#     decoderLSTM = LSTM(n_s, return_state = True) #decoder_LSTM_cell\n","    \n","#     for t in range(Ty):\n","#         # Perform one step of the attention mechanism to get back the context vector at step t\n","#         context, attention_scores, energies = one_step_attention(encoder_output, state_s)\n","        \n","#         # Apply the post-attention LSTM cell to the \"context\" vector.\n","#         # Decoder output 1. decoder output 2. decoder state 3. decoder cell state\n","#         state_s, _ , state_c = decoderLSTM(context, initial_state=[state_s, state_c])\n","        \n","#         # Apply Dense layer to the hidden state output of the post-attention LSTM\n","#         out = Dense(machine_vocab_size, activation=\"softmax\")(state_s)\n","        \n","#         # Append \"out\" to the \"outputs\" list\n","#         outputs.append(out)\n","        \n","#     # Create model instance\n","#     model = Model(inputs=[encoder_input, decoder_state_input_h, decoder_state_input_c], outputs=outputs)\n","    \n","#     return model"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["model = model(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size)\n","# model.summary()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# opt = Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n","opt = Adam(learning_rate=0.001) \n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# from keras.utils import plot_model\n","\n","# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Initialize s0 and c0\n","# With m = vocab_size, n_s = 64 hidden dimensions of the decoder\n","s0 = np.zeros((m, n_s))\n","c0 = np.zeros((m, n_s))\n","# Create a list of outputs\n","# We need to swap the axis because the model expects the output to be of shape (m, Ty, vocab_size)\n","outputs = list(Y.swapaxes(0,1))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Y shape: (10887, 40, 24) and outputs shape: (40, 10887, 24)\n"]}],"source":["print(f'Y shape: {Y.shape} and outputs shape: {np.array(outputs).shape}')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# model.fit([X, s0, c0], outputs, epochs=100, batch_size=64, verbose=1)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def inference_encoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_sizem):\n","   X =  Input(shape=(Tx, vocab_size))\n","   h = encoder_LSTM(X)\n","   model = Model(inputs=[X],outputs=h)\n","   return model\n","\n","def inference_decoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","    h = Input(shape=(inference_encoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size).output_shape[1], n_h*2))\n","    s0 = Input(shape=(n_s,), name='s0')\n","    c0 = Input(shape=(n_s,), name='c0')\n","    s = s0\n","    c = c0\n","    context, attention_scores, energies = one_step_attention(h, s)\n","    s, _, c = decoder_LSTM_cell(context, initial_state=[s, c])\n","    out = output_layer(s)\n","    model = Model(inputs=[h,s0,c0],outputs=[out,s,c,attention_scores,energies])\n","    return model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["inference_encoder_model = inference_encoder(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size)\n","inference_decoder_model = inference_decoder(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# model.save('fit_model')\n","# inference_encoder_model.save('inference_encoder_model')\n","# inference_decoder_model.save('inference_decoder_model')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **Load Model and Layer**"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["model = load_model('fit_model', custom_objects={'one_step_attention': one_step_attention})\n","inference_encoder_model = load_model('inference_encoder_model', custom_objects={'one_step_attention': one_step_attention})\n","inference_decoder_model = load_model('inference_decoder_model', custom_objects={'one_step_attention': one_step_attention})"]},{"cell_type":"markdown","metadata":{"id":"3C2RET9GwXDh"},"source":["# Thai-Script to Roman-Script Translation\n","* Task 4: Test your model on 5 examples of your choice including your name! \n","* Task 5: Show your visualization of attention scores on one of your example "]},{"cell_type":"code","execution_count":31,"metadata":{"id":"gON7T2xVwXDk"},"outputs":[],"source":["#task 4\n","#fill your code here\n","def prep_input(input_list):\n","    X = []\n","    for name in input_list:\n","        temp = []\n","        for char in name:\n","            temp.append(input_char2idx[char])\n","        X.append(temp)\n","    X = pad_sequences(maxlen=Tx, sequences=X, padding=\"post\", value=0)\n","    X = to_categorical(X, num_classes=vocab_size)\n","    X = X.reshape(len(input_list), Tx, vocab_size)\n","    return X"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 20, 65)\n"]}],"source":["EXAMPLES_ = [\"อธิเมศร์\",'สรพัศ',\"ธนัช\",\"มิก\",\"ซ้ง\",\"ออมซ์\",\"ไกรสีห์\",\"พัชรี\",\"เบิ้ล\",\"สิรวิชญ์\"]\n","X_ = prep_input(EXAMPLES_)\n","s0_ = np.zeros((len(EXAMPLES_), n_s))\n","c0_ = np.zeros((len(EXAMPLES_), n_s))\n","print(X_.shape)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 17s 17s/step\n","input:  อธิเมศร์ output:  athimat\n","input:  สรพัศ output:  sonphat\n","input:  ธนัช output:  thanat\n","input:  มิก output:  mik\n","input:  ซ้ง output:  song\n","input:  ออมซ์ output:  om\n","input:  ไกรสีห์ output:  kraisi\n","input:  พัชรี output:  phatri\n","input:  เบิ้ล output:  bon\n","input:  สิรวิชญ์ output:  sinwit\n"]}],"source":["pred_ = model.predict([X_, s0_, c0_])\n","pred_ = np.swapaxes(pred_, 0, 1)\n","pred_ = np.argmax(pred_, axis=-1)\n","for j in range(len(pred_)):\n","    temp = [c for c in pred_[j] if c != 0]\n","    output_ = ''.join([output_idx2char[i] for i in temp])\n","    print(\"input: \", EXAMPLES_[j], \"output: \", output_)"]},{"cell_type":"markdown","metadata":{"id":"r9-mxbsKwXDp"},"source":["### Plot the attention map\n","* If you need to install thai font: sudo apt install xfonts-thai\n","* this is what your visualization might look like:\n","--> https://drive.google.com/file/d/168J5SPSf4NNKj718wWUEDpUbh8QYZKux/view?usp=share_link"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"-4lhl4Vsz6Y8"},"outputs":[],"source":["# EXAMPLES = ???\n","# h = inferEncoder_model.predict(EXAMPLES)\n","# s0 = ???\n","# c0 = ???\n","# ...\n","# Ty = 10\n","# for t in range(Ty):\n","#   out,s,c,attention_scores,energies = inferDecoder_model.predict([h,s0,c0])\n","# ...\n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 20, 65)\n","[[[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n"]}],"source":["EXAMPLES_ = ['อธิเมศร์']\n","EXAMPLES = prep_input(EXAMPLES_)\n","print(EXAMPLES.shape)\n","print(EXAMPLES)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 26ms/step\n"]}],"source":["h = inference_encoder_model.predict(EXAMPLES)\n","s0 = np.zeros((len(EXAMPLES_), n_s))\n","c0 = np.zeros((len(EXAMPLES_), n_s))\n","list_attention = []\n","list_prediction = []"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 394ms/step\n","0 : input --> อ output --> a ( 3 )\n","[0.49363047 0.19670457 0.01923161 0.01728993 0.01707146 0.01707146\n"," 0.01707146 0.01707146 0.01707146 0.01707146 0.01707146 0.01707146\n"," 0.01707146 0.01707146 0.01707146 0.01707146 0.01707146 0.01707146\n"," 0.01707146 0.01707146]\n","1/1 [==============================] - 0s 27ms/step\n","1 : input --> ธ output --> t ( 20 )\n","[0.3577467  0.3481117  0.10213424 0.02013135 0.01156147 0.01077151\n"," 0.01066476 0.0109841  0.01064548 0.01064522 0.01064522 0.01064522\n"," 0.01064522 0.01064522 0.01064522 0.01064522 0.01064614 0.01066403\n"," 0.01069104 0.01073096]\n","1/1 [==============================] - 0s 27ms/step\n","2 : input --> ิ output --> h ( 10 )\n","[0.23649968 0.23628101 0.22177233 0.14899288 0.03127783 0.01031462\n"," 0.00798932 0.01549066 0.00759387 0.00721004 0.00710958 0.00708781\n"," 0.00710161 0.0071456  0.00723022 0.0073744  0.00760735 0.00797423\n"," 0.00854122 0.00940576]\n","1/1 [==============================] - 0s 33ms/step\n","3 : input --> เ output --> i ( 11 )\n","[0.18656683 0.18653467 0.1843086  0.1694012  0.0918365  0.02249403\n"," 0.01016695 0.04723258 0.00822366 0.00644777 0.00600534 0.00591086\n"," 0.00597069 0.00616281 0.00653785 0.00719265 0.00828821 0.01009109\n"," 0.01300961 0.01761805]\n","1/1 [==============================] - 0s 25ms/step\n","4 : input --> ม output --> m ( 14 )\n","[0.09477278 0.09477192 0.09471125 0.09427495 0.09046273 0.07301199\n"," 0.04654347 0.08441037 0.03369148 0.01442372 0.00854045 0.00731355\n"," 0.00808721 0.01062948 0.01560345 0.02358275 0.03423276 0.04614977\n"," 0.05755309 0.06723281]\n","1/1 [==============================] - 0s 24ms/step\n","5 : input --> ศ output --> a ( 3 )\n","[0.08422489 0.08422442 0.08419108 0.08395103 0.08182476 0.07134902\n"," 0.05231538 0.07833702 0.04099865 0.01951875 0.0112518  0.0094016\n"," 0.01057299 0.01430372 0.02105828 0.03060229 0.04151324 0.05199357\n"," 0.06080126 0.0675662 ]\n","1/1 [==============================] - 0s 28ms/step\n","6 : input --> ร output --> t ( 20 )\n","[0.09207773 0.09207699 0.09202445 0.09164647 0.08833083 0.07283862\n"," 0.04816416 0.08301814 0.03553161 0.01550447 0.00907388 0.00771565\n"," 0.00857266 0.01137424 0.01677539 0.02521015 0.03607403 0.04778433\n"," 0.05862311 0.06758309]\n","1/1 [==============================] - 0s 24ms/step\n","7 : input --> ์ output --> <PAD> ( 0 )\n","[0.20123339 0.20117804 0.19737037 0.17294815 0.07332611 0.01675817\n"," 0.00896581 0.0339968  0.00772868 0.00657324 0.00627944 0.0062163\n"," 0.0062563  0.00638435 0.00663271 0.00706189 0.00777014 0.00891785\n"," 0.01075333 0.01364898]\n","1/1 [==============================] - 0s 30ms/step\n","8 : input --> <PAD> output --> <PAD>\n","[0.38252935 0.36109188 0.05705168 0.01587386 0.01186289 0.01147034\n"," 0.01142394 0.01157659 0.01142394 0.01142394 0.01142394 0.01142394\n"," 0.01142394 0.01142394 0.01142394 0.01142394 0.01142394 0.01142394\n"," 0.01143    0.01145002]\n","1/1 [==============================] - 0s 27ms/step\n","9 : input --> <PAD> output --> <PAD>\n","[0.50590086 0.14259316 0.02081818 0.01956465 0.0194452  0.0194452\n"," 0.0194452  0.0194452  0.0194452  0.0194452  0.0194452  0.0194452\n"," 0.0194452  0.0194452  0.0194452  0.0194452  0.0194452  0.0194452\n"," 0.0194452  0.0194452 ]\n","1/1 [==============================] - 0s 30ms/step\n","10 : input --> <PAD> output --> <PAD>\n","[0.44120428 0.07385482 0.02743992 0.02691182 0.02691182 0.02691182\n"," 0.02691182 0.02691182 0.02691182 0.02691182 0.02691182 0.02691182\n"," 0.02691182 0.02691182 0.02691182 0.02691182 0.02691182 0.02691182\n"," 0.02691182 0.02691182]\n","1/1 [==============================] - 0s 27ms/step\n","11 : input --> <PAD> output --> <PAD>\n","[0.3249629  0.05909364 0.03445997 0.03420491 0.03420491 0.03420491\n"," 0.03420491 0.03420491 0.03420491 0.03420491 0.03420491 0.03420491\n"," 0.03420491 0.03420491 0.03420491 0.03420491 0.03420491 0.03420491\n"," 0.03420491 0.03420491]\n","1/1 [==============================] - 0s 26ms/step\n","12 : input --> <PAD> output --> <PAD>\n","[0.27580002 0.05669092 0.0372541  0.03707382 0.03707382 0.03707382\n"," 0.03707382 0.03707382 0.03707382 0.03707382 0.03707382 0.03707382\n"," 0.03707382 0.03707382 0.03707382 0.03707382 0.03707382 0.03707382\n"," 0.03707382 0.03707382]\n","1/1 [==============================] - 0s 26ms/step\n","13 : input --> <PAD> output --> <PAD>\n","[0.26944903 0.05644549 0.03761207 0.03744078 0.03744078 0.03744078\n"," 0.03744078 0.03744078 0.03744078 0.03744078 0.03744078 0.03744078\n"," 0.03744078 0.03744078 0.03744078 0.03744078 0.03744078 0.03744078\n"," 0.03744078 0.03744078]\n","1/1 [==============================] - 0s 26ms/step\n","14 : input --> <PAD> output --> <PAD>\n","[0.26736188 0.05636732 0.03772958 0.03756125 0.03756125 0.03756125\n"," 0.03756125 0.03756125 0.03756125 0.03756125 0.03756125 0.03756125\n"," 0.03756125 0.03756125 0.03756125 0.03756125 0.03756125 0.03756125\n"," 0.03756125 0.03756125]\n","1/1 [==============================] - 0s 25ms/step\n","15 : input --> <PAD> output --> <PAD>\n","[0.26418978 0.05625077 0.0379081  0.0377442  0.0377442  0.0377442\n"," 0.0377442  0.0377442  0.0377442  0.0377442  0.0377442  0.0377442\n"," 0.0377442  0.0377442  0.0377442  0.0377442  0.0377442  0.0377442\n"," 0.0377442  0.0377442 ]\n","1/1 [==============================] - 0s 27ms/step\n","16 : input --> <PAD> output --> <PAD>\n","[0.2595068  0.05608344 0.03817142 0.03801402 0.03801402 0.03801402\n"," 0.03801402 0.03801402 0.03801402 0.03801402 0.03801402 0.03801402\n"," 0.03801402 0.03801402 0.03801402 0.03801402 0.03801402 0.03801402\n"," 0.03801402 0.03801402]\n","1/1 [==============================] - 0s 25ms/step\n","17 : input --> <PAD> output --> <PAD>\n","[0.2532977  0.05586977 0.03852016 0.03837132 0.03837132 0.03837132\n"," 0.03837132 0.03837132 0.03837132 0.03837132 0.03837132 0.03837132\n"," 0.03837132 0.03837132 0.03837132 0.03837132 0.03837132 0.03837132\n"," 0.03837132 0.03837132]\n","1/1 [==============================] - 0s 35ms/step\n","18 : input --> <PAD> output --> <PAD>\n","[0.24606824 0.05563178 0.03892572 0.03878672 0.03878672 0.03878672\n"," 0.03878672 0.03878672 0.03878672 0.03878672 0.03878672 0.03878672\n"," 0.03878672 0.03878672 0.03878672 0.03878672 0.03878672 0.03878672\n"," 0.03878672 0.03878672]\n","1/1 [==============================] - 0s 25ms/step\n","19 : input --> <PAD> output --> <PAD>\n","[0.23825574 0.05538641 0.03936341 0.03923497 0.03923497 0.03923497\n"," 0.03923497 0.03923497 0.03923497 0.03923497 0.03923497 0.03923497\n"," 0.03923497 0.03923497 0.03923497 0.03923497 0.03923497 0.03923497\n"," 0.03923497 0.03923497]\n","1/1 [==============================] - 0s 25ms/step\n","20 : input --> <PAD> output --> <PAD>\n","[0.23006538 0.05514093 0.0398217  0.03970423 0.03970423 0.03970423\n"," 0.03970423 0.03970423 0.03970423 0.03970423 0.03970423 0.03970423\n"," 0.03970423 0.03970423 0.03970423 0.03970423 0.03970423 0.03970423\n"," 0.03970423 0.03970423]\n","1/1 [==============================] - 0s 25ms/step\n","21 : input --> <PAD> output --> <PAD>\n","[0.22194348 0.05490793 0.04027568 0.040169   0.040169   0.040169\n"," 0.040169   0.040169   0.040169   0.040169   0.040169   0.040169\n"," 0.040169   0.040169   0.040169   0.040169   0.040169   0.040169\n"," 0.040169   0.040169  ]\n","1/1 [==============================] - 0s 25ms/step\n","22 : input --> <PAD> output --> <PAD>\n","[0.21398565 0.05468841 0.04072006 0.04062388 0.04062388 0.04062388\n"," 0.04062388 0.04062388 0.04062388 0.04062388 0.04062388 0.04062388\n"," 0.04062388 0.04062388 0.04062388 0.04062388 0.04062388 0.04062388\n"," 0.04062388 0.04062388]\n","1/1 [==============================] - 0s 23ms/step\n","23 : input --> <PAD> output --> <PAD>\n","[0.20640483 0.05448625 0.04114303 0.04105681 0.04105681 0.04105681\n"," 0.04105681 0.04105681 0.04105681 0.04105681 0.04105681 0.04105681\n"," 0.04105681 0.04105681 0.04105681 0.04105681 0.04105681 0.04105681\n"," 0.04105681 0.04105681]\n","1/1 [==============================] - 0s 25ms/step\n","24 : input --> <PAD> output --> <PAD>\n","[0.19899242 0.05429417 0.04155633 0.04147983 0.04147983 0.04147983\n"," 0.04147983 0.04147983 0.04147983 0.04147983 0.04147983 0.04147983\n"," 0.04147983 0.04147983 0.04147983 0.04147983 0.04147983 0.04147983\n"," 0.04147983 0.04147983]\n","1/1 [==============================] - 0s 24ms/step\n","25 : input --> <PAD> output --> <PAD>\n","[0.1919792  0.05411666 0.04194713 0.04187982 0.04187982 0.04187982\n"," 0.04187982 0.04187982 0.04187982 0.04187982 0.04187982 0.04187982\n"," 0.04187982 0.04187982 0.04187982 0.04187982 0.04187982 0.04187982\n"," 0.04187982 0.04187982]\n","1/1 [==============================] - 0s 27ms/step\n","26 : input --> <PAD> output --> <PAD>\n","[0.18528487 0.05395033 0.04232001 0.04226145 0.04226145 0.04226145\n"," 0.04226145 0.04226145 0.04226145 0.04226145 0.04226145 0.04226145\n"," 0.04226145 0.04226145 0.04226145 0.04226145 0.04226145 0.04226145\n"," 0.04226145 0.04226145]\n","1/1 [==============================] - 0s 36ms/step\n","27 : input --> <PAD> output --> <PAD>\n","[0.17898245 0.05379592 0.04267093 0.04262063 0.04262063 0.04262063\n"," 0.04262063 0.04262063 0.04262063 0.04262063 0.04262063 0.04262063\n"," 0.04262063 0.04262063 0.04262063 0.04262063 0.04262063 0.04262063\n"," 0.04262063 0.04262063]\n","1/1 [==============================] - 0s 25ms/step\n","28 : input --> <PAD> output --> <PAD>\n","[0.1730266  0.05365143 0.04300247 0.04295997 0.04295997 0.04295997\n"," 0.04295997 0.04295997 0.04295997 0.04295997 0.04295997 0.04295997\n"," 0.04295997 0.04295997 0.04295997 0.04295997 0.04295997 0.04295997\n"," 0.04295997 0.04295997]\n","1/1 [==============================] - 0s 24ms/step\n","29 : input --> <PAD> output --> <PAD>\n","[0.167419   0.05351622 0.04331454 0.04327942 0.04327942 0.04327942\n"," 0.04327942 0.04327942 0.04327942 0.04327942 0.04327942 0.04327942\n"," 0.04327942 0.04327942 0.04327942 0.04327942 0.04327942 0.04327942\n"," 0.04327942 0.04327942]\n","1/1 [==============================] - 0s 31ms/step\n","30 : input --> <PAD> output --> <PAD>\n","[0.16223183 0.05339153 0.04360319 0.04357491 0.04357491 0.04357491\n"," 0.04357491 0.04357491 0.04357491 0.04357491 0.04357491 0.04357491\n"," 0.04357491 0.04357491 0.04357491 0.04357491 0.04357491 0.04357491\n"," 0.04357491 0.04357491]\n","1/1 [==============================] - 0s 23ms/step\n","31 : input --> <PAD> output --> <PAD>\n","[0.15722276 0.05327111 0.0438819  0.04386025 0.04386025 0.04386025\n"," 0.04386025 0.04386025 0.04386025 0.04386025 0.04386025 0.04386025\n"," 0.04386025 0.04386025 0.04386025 0.04386025 0.04386025 0.04386025\n"," 0.04386025 0.04386025]\n","1/1 [==============================] - 0s 25ms/step\n","32 : input --> <PAD> output --> <PAD>\n","[0.15256566 0.05315893 0.04414101 0.04412555 0.04412555 0.04412555\n"," 0.04412555 0.04412555 0.04412555 0.04412555 0.04412555 0.04412555\n"," 0.04412555 0.04412555 0.04412555 0.04412555 0.04412555 0.04412555\n"," 0.04412555 0.04412555]\n","1/1 [==============================] - 0s 23ms/step\n","33 : input --> <PAD> output --> <PAD>\n","[0.14817744 0.05305278 0.04438517 0.04437556 0.04437556 0.04437556\n"," 0.04437556 0.04437556 0.04437556 0.04437556 0.04437556 0.04437556\n"," 0.04437556 0.04437556 0.04437556 0.04437556 0.04437556 0.04437556\n"," 0.04437556 0.04437556]\n","1/1 [==============================] - 0s 25ms/step\n","34 : input --> <PAD> output --> <PAD>\n","[0.14398547 0.05295077 0.04461842 0.04461443 0.04461443 0.04461443\n"," 0.04461443 0.04461443 0.04461443 0.04461443 0.04461443 0.04461443\n"," 0.04461443 0.04461443 0.04461443 0.04461443 0.04461443 0.04461443\n"," 0.04461443 0.04461443]\n","1/1 [==============================] - 0s 28ms/step\n","35 : input --> <PAD> output --> <PAD>\n","[0.14010833 0.05285565 0.04483534 0.04483534 0.04483534 0.04483534\n"," 0.04483534 0.04483534 0.04483534 0.04483534 0.04483534 0.04483534\n"," 0.04483534 0.04483534 0.04483534 0.04483534 0.04483534 0.04483534\n"," 0.04483534 0.04483534]\n","1/1 [==============================] - 0s 27ms/step\n","36 : input --> <PAD> output --> <PAD>\n","[0.1364313  0.05276449 0.04504468 0.04504468 0.04504468 0.04504468\n"," 0.04504468 0.04504468 0.04504468 0.04504468 0.04504468 0.04504468\n"," 0.04504468 0.04504468 0.04504468 0.04504468 0.04504468 0.04504468\n"," 0.04504468 0.04504468]\n","1/1 [==============================] - 0s 24ms/step\n","37 : input --> <PAD> output --> <PAD>\n","[0.13287178 0.05267538 0.04524738 0.04524738 0.04524738 0.04524738\n"," 0.04524738 0.04524738 0.04524738 0.04524738 0.04524738 0.04524738\n"," 0.04524738 0.04524738 0.04524738 0.04524738 0.04524738 0.04524738\n"," 0.04524738 0.04524738]\n","1/1 [==============================] - 0s 28ms/step\n","38 : input --> <PAD> output --> <PAD>\n","[0.12949242 0.0525899  0.04543987 0.04543987 0.04543987 0.04543987\n"," 0.04543987 0.04543987 0.04543987 0.04543987 0.04543987 0.04543987\n"," 0.04543987 0.04543987 0.04543987 0.04543987 0.04543987 0.04543987\n"," 0.04543987 0.04543987]\n","1/1 [==============================] - 0s 29ms/step\n","39 : input --> <PAD> output --> <PAD>\n","[0.12634417 0.05250937 0.04561925 0.04561925 0.04561925 0.04561925\n"," 0.04561925 0.04561925 0.04561925 0.04561925 0.04561925 0.04561925\n"," 0.04561925 0.04561925 0.04561925 0.04561925 0.04561925 0.04561925\n"," 0.04561925 0.04561925]\n"]}],"source":["for t in range(Ty):\n","    out,s,c,attention_scores,energies = inference_decoder_model.predict([h,s0,c0])\n","    s0 = s\n","    c0 = c\n","    pred = np.argmax(np.swapaxes(out, 0, 1), axis=0)[0]\n","    if t < len(EXAMPLES_[0]):\n","        print(t, \": input -->\", EXAMPLES_[0][t], \"output -->\", output_idx2char[pred], '(',pred,')')\n","    else:\n","        print(t, \": input -->\", \"<PAD>\", \"output -->\", output_idx2char[pred])\n","    print(attention_scores.reshape(-1))\n","    list_attention.append(attention_scores.reshape(-1))\n","    list_prediction.append(output_idx2char[pred])"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["athimat<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"]}],"source":["print(''.join(list_prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRL8hHaLwXDq"},"outputs":[],"source":["#task 5\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.rcParams['font.family']='TH Sarabun New'  #you can change to other font that works for you\n","#fill your code here"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
