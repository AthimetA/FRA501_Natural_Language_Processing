{"cells":[{"cell_type":"markdown","metadata":{"id":"Y1cDcKRZwXCL"},"source":["# Key-Value Attention Mechanism Homework on Keras: Character-level Machine Translation (Many-to-Many, encoder-decoder)\n","\n","In this homework, you will create an MT model with key-value attention mechnism that coverts names of constituency MP candidates in the 2019 Thai general election from Thai script to Roman(Latin) script. E.g. นิยม-->niyom "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6024,"status":"ok","timestamp":1680718678537,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Oy6QYsP4wa-k","outputId":"8ac7d294-ec86-484c-d1cb-af31f6665b9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.10.1\n"]}],"source":["# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","import matplotlib as mpl\n","import matplotlib.font_manager as fm\n","\n","fm.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n","mpl.rc('font', family='TH Sarabun New')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import keras\n","import numpy as np\n","import random\n","np.random.seed(0)\n","\n","from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from keras.layers import RepeatVector, Dense, Activation, Lambda\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical, pad_sequences\n","from keras.models import load_model, Model\n","from keras import backend as K\n","\n","# %matplotlib inline\n","# from tensorflow.keras.preprocessing.sequence import pad_sequences\n","# from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","# from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras.utils import to_categorical\n","# from tensorflow.keras import Model\n","# from tensorflow.keras.models import load_model\n","# import tensorflow.keras.backend as K\n","# import numpy as np\n","\n","# import random"]},{"cell_type":"markdown","metadata":{"id":"Sq20keO6wXCh"},"source":["## Load Dataset\n","We have generated a toy dataset using names of constituency MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n","\n","<img src=\"https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/images/dataset_diagram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78675,"status":"ok","timestamp":1680719078872,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"B8ILK0shprTa","outputId":"55715c6f-dfe1-4bf5-ad12-a021e5f8018a"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# import shutil\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/mp_name_th_en.csv\", \"/content/mp_name_th_en.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rTQk8W4OwXCk"},"outputs":[],"source":["import csv\n","with open('data/mp_name_th_en.csv') as csvfile:\n","    readCSV = csv.reader(csvfile, delimiter=',')\n","    name_th = []\n","    name_en = []\n","    for row in readCSV:\n","        name_th.append(row[0])\n","        name_en.append(row[1])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680719134852,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"lVNHVM_FwXCs","outputId":"1b5c8a4d-5d04-4266-e734-5e199bedaf05"},"outputs":[{"name":"stdout","output_type":"stream","text":["ไกรสีห์ kraisi\n","พัชรี phatri\n","ธีระ thira\n","วุฒิกร wutthikon\n","ไสว sawai\n","สัมภาษณ์  samphat\n","วศิน wasin\n","ทินวัฒน์ thinwat\n","ศักดินัย sakdinai\n","สุรศักดิ์ surasak\n"]}],"source":["for th, en in zip(name_th[:10],name_en[:10]):\n","    print(th,en)"]},{"cell_type":"markdown","metadata":{"id":"heMTiM7qwXC2"},"source":["## Task1: Preprocess dataset for Keras\n","* 2 dictionaries for indexing (1 for input and another for output)\n","* DON'T FORGET TO INCLUDE special token for padding\n","* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)\n","* Be mindful of your pad_sequences \"padding\" hyperparameter. Choose wisely (post-padding vs pre-padding)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_O5YhjntwXC4"},"outputs":[{"name":"stdout","output_type":"stream","text":["data has 10887 names, 65 unique characters in input, 24 unique characters in output.\n","max input length is 20\n"]}],"source":["#FILL YOUR CODE HERE\n","\n","input_chars = list(set(''.join(name_th)))\n","output_chars = list(set(''.join(name_en)))\n","\n","data_size, vocab_size = len(name_th), len(input_chars)+1 # 1 for padding\n","output_size, output_vocab_size = len(name_en), len(output_chars)+ 2 # 1 for padding 1 for end of word\n","\n","print('data has %d names, %d unique characters in input, %d unique characters in output.' % (data_size, vocab_size, output_vocab_size))\n","maxlen = len( max(name_th, key=len)) #max input length\n","print('max input length is', maxlen)\n","\n","m = data_size"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input_char_indices {0: '<PAD>', 1: ' ', 2: 'ก', 3: 'ข', 4: 'ค', 5: 'ฆ', 6: 'ง', 7: 'จ', 8: 'ฉ', 9: 'ช', 10: 'ซ', 11: 'ฌ', 12: 'ญ', 13: 'ฎ', 14: 'ฏ', 15: 'ฐ', 16: 'ฑ', 17: 'ฒ', 18: 'ณ', 19: 'ด', 20: 'ต', 21: 'ถ', 22: 'ท', 23: 'ธ', 24: 'น', 25: 'บ', 26: 'ป', 27: 'ผ', 28: 'ฝ', 29: 'พ', 30: 'ฟ', 31: 'ภ', 32: 'ม', 33: 'ย', 34: 'ร', 35: 'ล', 36: 'ว', 37: 'ศ', 38: 'ษ', 39: 'ส', 40: 'ห', 41: 'ฬ', 42: 'อ', 43: 'ฮ', 44: 'ะ', 45: 'ั', 46: 'า', 47: 'ำ', 48: 'ิ', 49: 'ี', 50: 'ึ', 51: 'ื', 52: 'ุ', 53: 'ู', 54: 'เ', 55: 'แ', 56: 'โ', 57: 'ใ', 58: 'ไ', 59: '็', 60: '่', 61: '้', 62: '๊', 63: '๋', 64: '์'}\n","output_char_indices {0: '<PAD>', 1: '<EOS>', 2: '-', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'w', 23: 'y'}\n"]}],"source":["sorted_input_chars = sorted(input_chars)\n","sorted_output_chars = sorted(output_chars)\n","\n","sorted_input_chars.insert(0,\"<PAD>\") #PADDING\n","sorted_output_chars.insert(0,\"<EOS>\") #END OF WORD\n","sorted_output_chars.insert(0,\"<PAD>\") #PADDING\n","\n","# Input dictionary\n","input_char2idx = dict((c, i) for i, c in enumerate(sorted_input_chars))\n","input_idx2char = dict((i, c) for i, c in enumerate(sorted_input_chars))\n","# Output dictionary\n","output_char2idx = dict((c, i) for i, c in enumerate(sorted_output_chars))\n","output_idx2char = dict((i, c) for i, c in enumerate(sorted_output_chars))\n","\n","print('input_char_indices', input_idx2char)\n","print('output_char_indices', output_idx2char)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["samplesize 10887\n","max input length is 20\n","max output length is 40\n"]}],"source":["print('samplesize', data_size)\n","Tx = maxlen\n","# Ty = len(max(name_en, key=len)) # max output length\n","Ty = Tx * 2\n","print('max input length is', Tx)\n","print('max output length is', Ty)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Befor padding:\n","Sample X: [58, 2, 34, 39, 49, 40, 64] -> ไกรสีห์\n","Sample Y: [12, 18, 3, 11, 19, 11] -> kraisi\n","After padding:\n","Sample X: [58  2 34 39 49 40 64  0  0  0  0  0  0  0  0  0  0  0  0  0] -> ไกรสีห์<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n","Sample Y: [12 18  3 11 19 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] -> kraisi<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"]}],"source":["# Padding\n","X = []\n","for name in name_th:\n","    temp = []\n","    for char in name:\n","        temp.append(input_char2idx[char])\n","    X.append(temp)\n","\n","Y = []\n","for name in name_en:\n","    temp = []\n","    for char in name:\n","        temp.append(output_char2idx[char])\n","    Y.append(temp)\n","\n","print(f'Befor padding:')\n","xs = ''.join([input_idx2char[i] for i in X[0]])\n","print(f'Sample X: {X[0]} -> {xs}')\n","ys = ''.join([output_idx2char[i] for i in Y[0]])\n","print(f'Sample Y: {Y[0]} -> {ys}')\n","\n","# We choose padding='post' because we want to pad after the sentence\n","X = pad_sequences(X, maxlen=Tx, padding='post', value=0)\n","Y = pad_sequences(Y, maxlen=Ty, padding='post', value=0)\n","\n","print(f'After padding:')\n","xs = ''.join([input_idx2char[i] for i in X[0]])\n","print(f'Sample X: {X[0]} -> {xs}')\n","ys = ''.join([output_idx2char[i] for i in Y[0]]) \n","print(f'Sample Y: {Y[0]} -> {ys}')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X shape: (10887, 20, 65)\n","Y shape: (10887, 40, 24)\n"]}],"source":["# One-hot encoding\n","X = to_categorical(X, num_classes=vocab_size)\n","Y = to_categorical(Y, num_classes=output_vocab_size)\n","\n","X = X.reshape(data_size, Tx, vocab_size)\n","Y = Y.reshape(data_size, Ty, output_vocab_size)\n","\n","print(f'X shape: {X.shape}')\n","print(f'Y shape: {Y.shape}')"]},{"cell_type":"markdown","metadata":{"id":"HNqqnkVSwXC-"},"source":["# Attention Mechanism\n","## Task 2: Code your own (key-value) attention mechnism\n","* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n","* Define global variables\n","* fill code for one_step_attention function\n","* Hint: use keras.layers.Lambda \n","* Hint: you will probably need more hidden dimmensions than what you've seen in the demo\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"eSdFcuGuwXDB"},"outputs":[],"source":["from keras.activations import softmax\n","from keras.layers import Lambda\n","\n","def softMaxAxis1(x):\n","    return softmax(x,axis=1)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"BS3Ziti1wXDH"},"outputs":[],"source":["#These are global variables (shared layers)\n","## Fill your code here\n","## you are allowed to use code in the demo as your template. \n","\n","#repeater = ???\n","#concatenator = ???\n","repeator = RepeatVector(Tx, name='repeatorA')\n","concatenator = Concatenate(axis=-1, name='concatenatorA')\n","\n","#Key-values (Hint)\n","splitter = Lambda(lambda x:tf.split(x, num_or_size_splits=2,axis=2), name='splitterA') \n","\n","#fatten_1 = ???\n","#fatten_2 = ???\n","fatten_1 = Dense(1, activation = \"tanh\", name = \"fatten_1A\")\n","fatten_2 = Dense(1, activation = \"relu\", name = \"fatten_2A\")\n","\n","#activator = ???\n","activator = Activation(softMaxAxis1, name='attention_scoreA') # We are using a custom softmax(axis = 1) loaded in this notebook\n","#dotor = ???\n","dotor = Dot(axes = 1, name='dotorA')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ecNci8x5wXDN"},"outputs":[],"source":["def one_step_attention(a, s_prev):\n","\n","    # #Fill code here\n","    # #key, value = ???\n","    # key, value = splitter(a)\n","    # #concat = ...key...\n","    # # Repeat the decoder hidden state to concat with encoder hidden states\n","    # s_prev = repeator(s_prev)\n","    # concat = concatenator([key, s_prev])\n","    # # Attention function\n","    # e = fatten_1(concat)\n","    # energies = fatten_2(e)\n","    # # Calculate attention weights\n","    # alphas = attention_scores = activator(energies)\n","    # #context = ...value...\n","    # context = dotor([alphas, value])\n","\n","    # From Equation in the slides a = h (hidden state)(input), s_prev = s_{t-1} (Previous hidden state)\n","    # Get Key and Value from a (h) = [key, value]\n","    [key, value] = splitter(a)\n","\n","    # Find M and pass it through a tanh layer\n","    # M is the concatenation of the previous hidden state and the current hidden state\n","    s_prev = repeator(s_prev)\n","    concat = concatenator([key, s_prev])\n","    e = fatten_1(concat)\n","\n","    # Find energies and pass it through a relu layer\n","    energies = fatten_2(e)\n","\n","    # Calculate attention scores through softmax\n","    attention_scores = activator(energies)\n","\n","    # Find context vector\n","    context = dotor([attention_scores, value])\n"," \n","    return context, attention_scores , energies # return whatever you need to complete this homework "]},{"cell_type":"markdown","metadata":{"id":"2bgSCY3NwXDU"},"source":["## Task3: Create and train your encoder/decoder model here\n","* HINT: you will probably need more hidden dimmensions than what you've seen in the demo"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3CVrgh9nwXDV"},"outputs":[],"source":["#FILL CODE HERE :Hint --> heatmap in CNN + GradCAM\n","\n","# def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#   ...\n","\n","# def inference_encoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#    X = ....\n","#    h = ....\n","\n","#    model = Model(inputs=[X],outputs=h)\n","#    return model\n","\n","# def inference_decoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","#   s0 = ...\n","#   c0 = ...\n","#   h = ...\n","#   context, attention_scores, energies = one_step_attention(h, s)\n","#   ...decoder_LSTM_cell...\n","#   out = output_layer(s)\n","\n","#   model = Model(inputs=[h,s0,c0],outputs=[out,s,c,atten_score,energies])\n","\n","#   return model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["n_h = 32 #hidden dimensions for encoder \n","n_s = 64 #hidden dimensions for decoder\n","encoder_LSTM =  Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(-1, Tx, n_h*2), name = 'encoder_LSTM') #encoder_LSTM\n","decoder_LSTM_cell = LSTM(n_s, return_state = True, name='decoder_LSTM_cell') #decoder_LSTM_cell\n","output_layer = Dense(output_vocab_size, activation=\"softmax\", name='output_layerA') #output_layer"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","    \"\"\"\n","    Arguments:\n","    Tx -- length of the input sequence\n","    Ty -- length of the output sequence\n","    n_h -- hidden state size of the Bi-LSTM\n","    n_s -- hidden state size of the post-attention LSTM\n","    vocab_size -- size of the input vocab\n","    output_vocab_size -- size of the output vocab\n","\n","    Returns:\n","    model -- Keras model instance\n","    \"\"\"\n","    # Define the input of your model\n","    X = Input(shape=(Tx, vocab_size), name='Encoder_Input')\n","    # Define hidden state and cell state for decoder_LSTM_Cell\n","    s0 = Input(shape=(n_s,), name='s0')\n","    c0 = Input(shape=(n_s,), name='c0')\n","    s = s0\n","    c = c0\n","    \n","    # Initialize empty list of outputs\n","    outputs = list()\n","    \n","    # Encoder Bi-LSTM\n","    h = encoder_LSTM(X)\n","    \n","    # Iterate for Ty steps\n","    for t in range(Ty):\n","        # Perform one step of the attention mechanism to get back the context vector at step t\n","        context, attention_scores, energies = one_step_attention(h, s)\n","        \n","        # Apply the post-attention LSTM cell to the \"context\" vector.\n","        s, _, c = decoder_LSTM_cell(context, initial_state=[s, c])\n","        \n","        # Apply Dense layer to the hidden state output of the post-attention LSTM\n","        out = output_layer(s)\n","        \n","        # Append \"out\" to the \"outputs\" list\n","        outputs.append(out)\n","    \n","    # Create model instance\n","    model = Model(inputs=[X, s0, c0], outputs=outputs)\n","    \n","    return model"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"XptuOQj-wXDb"},"outputs":[],"source":["#FIT YOUR MODEL HERE"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n","    \n","#     # Initialize empty list of outputs\n","#     outputs = list()\n","    \n","#     # Set up encoder \n","#     encoder_input = Input(shape=(Tx, vocab_size))\n","#     encoderLSTM = Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(-1, Tx, n_h*2))\n","#     # Encoder output 1. encoder_output, 2. encoder_hidden_state, 3. encoder_cell_state\n","#     encoder_output, state_h, state_c = encoderLSTM(encoder_input)\n","    \n","#     # Define encoder states\n","#     encoder_states = [state_h, state_c]\n","    \n","#     # Set up decoder\n","\n","#     # Define decoder initial states\n","#     decoder_state_input_h = Input(shape=(n_s,), name='s0')\n","#     decoder_state_input_c = Input(shape=(n_s,), name='c0')\n","#     state_s = decoder_state_input_h\n","#     state_c = decoder_state_input_c\n","    \n","#     # Set up decoder LSTM\n","#     decoderLSTM = LSTM(n_s, return_state = True) #decoder_LSTM_cell\n","    \n","#     for t in range(Ty):\n","#         # Perform one step of the attention mechanism to get back the context vector at step t\n","#         context, attention_scores, energies = one_step_attention(encoder_output, state_s)\n","        \n","#         # Apply the post-attention LSTM cell to the \"context\" vector.\n","#         # Decoder output 1. decoder output 2. decoder state 3. decoder cell state\n","#         state_s, _ , state_c = decoderLSTM(context, initial_state=[state_s, state_c])\n","        \n","#         # Apply Dense layer to the hidden state output of the post-attention LSTM\n","#         out = Dense(machine_vocab_size, activation=\"softmax\")(state_s)\n","        \n","#         # Append \"out\" to the \"outputs\" list\n","#         outputs.append(out)\n","        \n","#     # Create model instance\n","#     model = Model(inputs=[encoder_input, decoder_state_input_h, decoder_state_input_c], outputs=outputs)\n","    \n","#     return model"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["model = model(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size)\n","# model.summary()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# opt = Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n","opt = Adam(learning_rate=0.1)\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# from keras.utils import plot_model\n","\n","# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Initialize s0 and c0\n","# With m = vocab_size, n_s = 64 hidden dimensions of the decoder\n","s0 = np.zeros((m, n_s))\n","c0 = np.zeros((m, n_s))\n","# Create a list of outputs\n","# We need to swap the axis because the model expects the output to be of shape (m, Ty, vocab_size)\n","outputs = list(Y.swapaxes(0,1))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Y shape: (10887, 40, 24) and outputs shape: (40, 10887, 24)\n"]}],"source":["print(f'Y shape: {Y.shape} and outputs shape: {np.array(outputs).shape}')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# model.fit([X, s0, c0], outputs, epochs=30, batch_size=100, verbose=1)\n","model.fit([X, s0, c0], outputs, epochs=10, batch_size=100, verbose=1)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["model.save('model_B')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **Load Model and Layer**"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# model = load_model('model', custom_objects={'one_step_attention': one_step_attention})\n","model = load_model('model_A', custom_objects={'one_step_attention': one_step_attention})"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["def inference_encoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_sizem, model):\n","   X =  Input(shape=(Tx, vocab_size))\n","   h = encoder_LSTM(X)\n","   model = Model(inputs=[X],outputs=h)\n","   return model\n","\n","def inference_decoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size, model):\n","    h = Input(shape=(inference_encoder(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size).output_shape[1], n_h*2))\n","    s0 = Input(shape=(n_s,), name='s0')\n","    c0 = Input(shape=(n_s,), name='c0')\n","    s = s0\n","    c = c0\n","    context, attention_scores, energies = one_step_attention(h, s)\n","    s, _, c = decoder_LSTM_cell(context, initial_state=[s, c])\n","    out = output_layer(s)\n","    model = Model(inputs=[h,s0,c0],outputs=[out,s,c,attention_scores,energies])\n","    return model"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["inference_encoder_model = inference_encoder(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size, model)\n","inference_decoder_model = inference_decoder(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size, model)"]},{"cell_type":"markdown","metadata":{"id":"3C2RET9GwXDh"},"source":["# Thai-Script to Roman-Script Translation\n","* Task 4: Test your model on 5 examples of your choice including your name! \n","* Task 5: Show your visualization of attention scores on one of your example "]},{"cell_type":"code","execution_count":36,"metadata":{"id":"gON7T2xVwXDk"},"outputs":[],"source":["#task 4\n","#fill your code here\n","def prep_input(input_list):\n","    X = []\n","    for name in input_list:\n","        temp = []\n","        for char in name:\n","            temp.append(input_char2idx[char])\n","        X.append(temp)\n","    X = pad_sequences(maxlen=Tx, sequences=X, padding=\"post\", value=0)\n","    X = to_categorical(X, num_classes=vocab_size)\n","    X = X.reshape(len(input_list), Tx, vocab_size)\n","    return X"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 20, 65)\n"]}],"source":["EXAMPLES_ = [\"อธิเมศร์\",'สรพัศ',\"ธนัช\",\"มิก\",\"ซ้ง\",\"ออมซ์\",\"ไกรสีห์\",\"พัชรี\",\"เบิ้ล\",\"สิรวิชญ์\"]\n","X_ = prep_input(EXAMPLES_)\n","s0_ = np.zeros((len(EXAMPLES_), n_s))\n","c0_ = np.zeros((len(EXAMPLES_), n_s))\n","print(X_.shape)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 17s 17s/step\n","input:  อธิเมศร์ output:  atiiiat\n","input:  สรพัศ output:  sunphap\n","input:  ธนัช output:  thanat\n","input:  มิก output:  mam\n","input:  ซ้ง output:  sang\n","input:  ออมซ์ output:  amom\n","input:  ไกรสีห์ output:  phaiiii\n","input:  พัชรี output:  phatchii\n","input:  เบิ้ล output:  banon\n","input:  สิรวิชญ์ output:  sirawit\n"]}],"source":["pred_ = model.predict([X_, s0_, c0_])\n","pred_ = np.swapaxes(pred_, 0, 1)\n","pred_ = np.argmax(pred_, axis=-1)\n","for j in range(len(pred_)):\n","    temp = [c for c in pred_[j] if c != 0]\n","    output_ = ''.join([output_idx2char[i] for i in temp])\n","    print(\"input: \", EXAMPLES_[j], \"output: \", output_)"]},{"cell_type":"markdown","metadata":{"id":"r9-mxbsKwXDp"},"source":["### Plot the attention map\n","* If you need to install thai font: sudo apt install xfonts-thai\n","* this is what your visualization might look like:\n","--> https://drive.google.com/file/d/168J5SPSf4NNKj718wWUEDpUbh8QYZKux/view?usp=share_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4lhl4Vsz6Y8"},"outputs":[],"source":["# EXAMPLES = ???\n","# h = inferEncoder_model.predict(EXAMPLES)\n","# s0 = ???\n","# c0 = ???\n","# ...\n","# Ty = 10\n","# for t in range(Ty):\n","#   out,s,c,attention_scores,energies = inferDecoder_model.predict([h,s0,c0])\n","# ...\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 20, 65)\n","[[[0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n"]}],"source":["EXAMPLES_ = ['อธิเมศร์']\n","EXAMPLES = prep_input(EXAMPLES_)\n","print(EXAMPLES.shape)\n","print(EXAMPLES)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 630ms/step\n"]}],"source":["h = inference_encoder_model.predict(EXAMPLES)\n","s0 = np.zeros((len(EXAMPLES_), n_s))\n","c0 = np.zeros((len(EXAMPLES_), n_s))\n","list_attention = []\n","list_prediction = []"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 410ms/step\n","0 : input --> อ output --> a ( 3 )\n","[0.04999996 0.05       0.05       0.05       0.05       0.05\n"," 0.05       0.05       0.05       0.05       0.05       0.05\n"," 0.05       0.05       0.05       0.05       0.05       0.05\n"," 0.05       0.05      ]\n","1/1 [==============================] - 0s 31ms/step\n","1 : input --> ธ output --> k ( 12 )\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 31ms/step\n","2 : input --> ิ output --> r ( 18 )\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 38ms/step\n","3 : input --> เ output --> r ( 18 )\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 31ms/step\n","4 : input --> ม output --> r ( 18 )\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 27ms/step\n","5 : input --> ศ output --> a ( 3 )\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 30ms/step\n","6 : input --> ร output --> a ( 3 )\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 32ms/step\n","7 : input --> ์ output --> k ( 12 )\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 31ms/step\n","8 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 36ms/step\n","9 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 28ms/step\n","10 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 24ms/step\n","11 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 28ms/step\n","12 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","13 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 26ms/step\n","14 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","15 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 30ms/step\n","16 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 28ms/step\n","17 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 28ms/step\n","18 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","19 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 26ms/step\n","20 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 24ms/step\n","21 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 26ms/step\n","22 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 27ms/step\n","23 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 24ms/step\n","24 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 27ms/step\n","25 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 26ms/step\n","26 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","27 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 27ms/step\n","28 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","29 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 24ms/step\n","30 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","31 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 27ms/step\n","32 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","33 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 25ms/step\n","34 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 26ms/step\n","35 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 24ms/step\n","36 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 40ms/step\n","37 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 27ms/step\n","38 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n","1/1 [==============================] - 0s 26ms/step\n","39 : input --> <PAD> output --> d\n","[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n"," 0.05 0.05 0.05 0.05 0.05 0.05]\n"]}],"source":["for t in range(Ty):\n","    out,s,c,attention_scores,energies = inference_decoder_model.predict([h,s0,c0])\n","    s0 = s\n","    c0 = c\n","    pred = np.argmax(np.swapaxes(out, 0, 1), axis=0)[0]\n","    if t < len(EXAMPLES_[0]):\n","        print(t, \": input -->\", EXAMPLES_[0][t], \"output -->\", output_idx2char[pred], '(',pred,')')\n","    else:\n","        print(t, \": input -->\", \"<PAD>\", \"output -->\", output_idx2char[pred])\n","    print(attention_scores.reshape(-1))\n","    list_attention.append(attention_scores.reshape(-1))\n","    list_prediction.append(output_idx2char[pred])"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["akrrraakdddddddddddddddddddddddddddddddd\n"]}],"source":["print(''.join(list_prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRL8hHaLwXDq"},"outputs":[],"source":["#task 5\n","%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.rcParams['font.family']='TH Sarabun New'  #you can change to other font that works for you\n","#fill your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XT44RFuxqzBM"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
